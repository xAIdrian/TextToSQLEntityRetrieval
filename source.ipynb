{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep an eye on the progress in notion \n",
    "\n",
    "@October 14 https://www.notion.so/adrianmohnacs/3dcf69ad6da44496ab9f890044158553?pvs=4\n",
    "\n",
    "@October 15\n",
    "\n",
    "## Pipeline Steps:\n",
    "\n",
    "- Depenencies and file loading\n",
    "- Getting familiar with data\n",
    "- Data preparation. This involves mapping the entity type to fields and combining the user query with the field description. (field_Description provides the semantic context to get the correct json prop)\n",
    "- JSON cleaning and error handling\n",
    "- We then label the data using the json entities extracted to provide more context to the data.\n",
    "- Map dictionairies to dataset (this will change)\n",
    "- Training / validation split\n",
    "- Saving dataset locally\n",
    "- Loading or training the a new model\n",
    "- Save the model and tokenizer locally\n",
    "- Evaluation\n",
    "- Inference\n",
    "\n",
    "##  Merged data set for processing\n",
    "**To train we remove the json and field_name using those for labeling**\n",
    "| entity_name | json | field_name | field_type | description |\n",
    "|-------------|------|------------|------------|--------------------------------------------------------------|\n",
    "| CDR         | {'entityType': 'CDR', 'statements': [{'type': 'technology', 'value': '3G'}]} | ifc.ootb.CDR.callStatus | string | Status of the call: \"Successful\", \"Failed\", \"Busy\", etc. |\n",
    "| CDR         | {'entityType': 'Web Activity', 'statements': [{'type': 'platform', 'value': 'Reddit'}, {'type': 'time', 'value': 'yesterday'}, {'type': 'keyword', 'value': 'funny'}]} | ifc.CDR.caseCode | string | Unique code identifying a specific case |\n",
    "| CDR         | {'entityType': 'Investigation', 'statements': [{'type': 'status', 'value': ['open', 'closed']}]} | ifc.CDR.chatTopic | string | Topic or subject of discussion in the chat |\n",
    "| CDR         | {'entityType': 'Insight', 'statements': [{'type': 'relatedTo', 'value': 'Jane Doe'}]} | ifc.ootb.CDR.createDateTime | date | Date and time of record creation. |\n",
    "| CDR         | {'entityType': 'Web Activity', 'statements': [{'type': 'time', 'value': 'last day'}]} | ifc.ootb.CDR.direction | string | Direction of the call (incoming, outgoing) |\n",
    "\n",
    "## Model Selection\n",
    "#### The winner is Albert\n",
    "\n",
    "| Model Variant |\tNumber of Parameters |\tModel Size on Disk\n",
    "|---------------|---------------------|---------------------|\n",
    "albert-base-v2 |\t11M |\t~46 MB |\t12\t|\n",
    "\n",
    "**We then move to training**\n",
    "By combining the query with field descriptions, the model can better understand the semantic meaning of the entities involved, improving its ability to map queries to the correct JSON labels.\n",
    "\n",
    "Input for training Example:\n",
    "\n",
    "```\n",
    "Query: Find all calls made using 3G technology. \n",
    "Entity (Label): CDR. \n",
    "Fields: callStatus: Status of the call; createDateTime: Date and time of record creation.\n",
    "```\n",
    "Label Example:\n",
    "```\n",
    "\"CDR\"\n",
    "```\n",
    "If there is a relation target:\n",
    "```\n",
    "\"CDR|Phone\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](B_DSKq.gif \"segment\")\n",
    "#### Not helping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLDR\n",
    "\n",
    "We were able to get local model pipeline in place with evaluation and inference. Prompt engineering is not needed.  This is a good start. We get some great results but the size of the dataset risks overfitting.  \n",
    "\n",
    "Though I will need another day of deep work to get the pipeline to production quality and A LOT OF TESTING.  This will also allow me to add some niceties to the notebook so you can just \"plug and play\".\n",
    "\n",
    "To improve our output I'd like to get clear on what quality outputs look like and discuss the data architecture and the relations between all the features. I'd also like to see more example outputs to get a better sense of the model's desired behavior.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Let's clearly break down the relation between the user query and the fields.  I want to hear it entirely from your perspective?\n",
    "- We want to predict BOTH the entity type and the relation target type?\n",
    "- What do you consider a good output here for the prediction?\n",
    "\n",
    "### What's left to do?\n",
    "\n",
    "- Test cases aligned with the examples you'd like to see\n",
    "- Clean up the notebook and add comments\n",
    "- Look into ways to get a bit more realistic accuracy of the model\n",
    "- Allow for a new CSV to be uploaded in markdown Ui in notebook and assigned to a path variable that is processed in data step\n",
    "- Simplify model training configuration make sure everything is optimized to be run on the local machine and using steps, not a mix of steps and epochs.\n",
    "- Optimized local storage and defensive code to conserve resources and make sure we save and load properly\n",
    "- TEST CASES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: sentencepiece in ./env/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./env/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./env/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./env/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./env/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./env/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./env/lib/python3.11/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !python -m venv env\n",
    "# !source env/bin/activate  \n",
    "!pip install torch transformers scikit-learn pandas sentencepiece\n",
    "!pip install numpy==1.26.4 --force-reinstall\n",
    "# needed for training\n",
    "! pip install -U accelerate\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Case for Getting Familiar with Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample JSON-like data (you'll replace this with your CSV data)\n",
    "json_data = [\n",
    "    {\"entityType\": \"CDR\", \"relationTargetType\": \"Phone\"},\n",
    "    {\"entityType\": \"Report\", \"relationTargetType\": \"Malware\"}\n",
    "]\n",
    "\n",
    "# Example query from the user\n",
    "query = \"What SMS messages were sent from suspicious phones to 0549876543 containing 'urgent'?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to search for relevant entitties in teh JSON data\n",
    "def find_matching_entities(query, json_data):\n",
    "    matching_entities = []\n",
    "\n",
    "    for record in json_data:\n",
    "        entity_text = f\"{record['entityType']} {record['relationTargetType']}\"\n",
    "\n",
    "        #encode inputs for model\n",
    "        inputs = tokenizer(query, entity_text, return_tensors=\"pt\")\n",
    "\n",
    "        #run the model to get answer scores\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #get the start and end scores for the answer\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "        #extract the answer\n",
    "        predicted_entity = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    "        )\n",
    "\n",
    "        #if predicted entity is not empty, consider it a match\n",
    "        if predicted_entity.strip():\n",
    "            return record['entityType'], record['relationTargetType']\n",
    "\n",
    "    return None\n",
    "    # return list(set(matching_entities)) #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test example\n",
    "matching_entities = list(set(find_matching_entities(query, json_data)))\n",
    "print(matching_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using data sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_name                   field_name field_type  \\\n",
      "0         CDR      ifc.ootb.CDR.callStatus     string   \n",
      "1         CDR             ifc.CDR.caseCode     string   \n",
      "2         CDR            ifc.CDR.chatTopic     string   \n",
      "3         CDR  ifc.ootb.CDR.createDateTime       date   \n",
      "4         CDR       ifc.ootb.CDR.direction     string   \n",
      "\n",
      "                                         description  \n",
      "0  Status of the call: \"Successful\", \"Failed\", \"B...  \n",
      "1            Unique code identifying a specific case  \n",
      "2         Topic or subject of discussion in the chat  \n",
      "3                  Date and time of record creation.  \n",
      "4         Direction of the call (incoming, outgoing)  \n",
      "                                            question  \\\n",
      "0           Find all calls made using 3G technology.   \n",
      "1  List all Reddit comments posted yesterday with...   \n",
      "2  Show me investigations that are either open or...   \n",
      "3  Find all insights related to the witness Jane ...   \n",
      "4  List all web activities updated in the last da...   \n",
      "\n",
      "                                                json  \n",
      "0  {'entityType': 'CDR', 'statements': [{'type': ...  \n",
      "1  {'entityType': 'Web Activity', 'statements': [...  \n",
      "2  {'entityType': 'Investigation', 'statements': ...  \n",
      "3  {'entityType': 'Insight', 'statements': [{'typ...  \n",
      "4  {'entityType': 'Web Activity', 'statements': [...  \n",
      "test sample mapping\n",
      "[{'field_name': 'ifc.ootb.Participant.isSuspicious', 'description': 'Boolean auto-generated Indicator showing if the identifier was defined as suspicious by the users or is it a SIM-Swapper (defined by the platform)'}, {'field_name': 'ifc.ootb.Participant.targetName', 'description': 'this is target based SIGINT - (local police level) (unlike mass sigint in country level). Name of target like Yossi cohen'}, {'field_name': 'ifc.ootb.Phone.IMEI', 'description': 'International Mobile Equipment Identity of the phone. It is a unique 15-digit code. Each IMEI number is unique to a device and does not change, even if the SIM card is changed'}, {'field_name': 'ifc.ootb.Phone.IMSI', 'description': 'IMSI stands for International Mobile Subscriber Identity. It is a unique number associated with all cellular networks. It is used to identify the user of a cellular network and is a key part of any mobile network. The IMSI is stored on a SIM card and consists of up to 15 digits.The structure of the IMSI is as follows:Mobile Country Code (MCC): The first three digits represent the country code, identifying the country in which the carrier operates.Mobile Network Code (MNC): The next two or three digits identify the mobile network within the country.Mobile Subscriber Identification Number (MSIN): The remainder of the IMSI is a unique identifier for the subscriber within the network.'}, {'field_name': 'ifc.ootb.Phone.MSISDN', 'description': \"MSISDN or phone number is the phone number associated with a mobile network subscriber. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to make and receive calls, send and receive SMS, and utilize mobile data.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).\"}, {'field_name': 'ifc.ootb.Phone.title', 'description': 'Title or label associated with the phone (to display on system)'}, {'field_name': 'ifc.ootb.Participant.tlidDominantLanguage', 'description': 'Dominant language for the Telecommunications Interception Directive'}, {'field_name': 'ifc.ootb.Participant.aliasName', 'description': 'Alias name of the participant'}, {'field_name': 'ifc.ootb.Participant.firstActiveDate', 'description': 'Date when the participant was first active on SIGINT'}, {'field_name': 'ifc.ootb.Participant.lastActiveDate', 'description': 'Date when the participant was last active on SIGINT'}, {'field_name': 'ifc.Phone.accessCallingNumber', 'description': 'Calling number used to access the phone'}, {'field_name': 'ifc.ootb.Participant.deviceName', 'description': 'Name of the device used by the participant'}, {'field_name': 'ifc.ootb.Participant.isTarget', 'description': 'Indicates whether the identifier defined as a target, meaning content is visible (Under a court order)'}, {'field_name': 'ifc.Phone.type', 'description': 'Type of phone (either foreign, mobile, or landline)'}, {'field_name': 'ifc.ootb.Phone.IMSI.objectID', 'description': \"Object ID associated with the phone's IMSI\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/z10gkxp14_vbcbgjmpzd69200000gn/T/ipykernel_16637/2222115628.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  entity_to_field_mapping = fields_desc.groupby('entity_name').apply(lambda x: x[['field_name', 'description']].to_dict(orient='records')).to_dict()\n"
     ]
    }
   ],
   "source": [
    "# load data and create mapping into new dataframe\n",
    "# right now we are just using the user query \n",
    "### TODO files are static paths now.  we need to make them dynamic and maybe add a nice UI to select the file\n",
    "import pandas as pd\n",
    "\n",
    "fields_desc = pd.read_csv('fields_description.csv')\n",
    "user_queries = pd.read_csv('user_queries.csv')\n",
    "\n",
    "print(fields_desc.head())\n",
    "print(user_queries.head())\n",
    "\n",
    "# Create a dictionary mapping entity names to their field descriptions and properties\n",
    "# This groups the data by entity_name and creates a nested dictionary structure for easy access to field information for each entity type\n",
    "entity_to_field_mapping = fields_desc.groupby('entity_name').apply(lambda x: x[['field_name', 'description']].to_dict(orient='records')).to_dict()\n",
    "\n",
    "print('test sample mapping')\n",
    "print(entity_to_field_mapping.get('Phone', []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error decoding JSON for query: Which phones have been marked as suspicious?\n",
      "Error: Expecting value: line 1 column 150 (char 149)\n",
      "Error decoding JSON for query: What failed call attempts were made from target phones to numbers containing '1234'?\n",
      "Error: Expecting value: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: Which phones are set on Arabic and are marked as suspicious?\n",
      "Error: Expecting value: line 1 column 280 (char 279)\n",
      "Error decoding JSON for query: List emails sent to phones associated with the target Sarah Johnson\n",
      "Error: Expecting value: line 1 column 337 (char 336)\n",
      "Error decoding JSON for query: List all emails from john@company.com to jane@company.com with attachments in February 2024\n",
      "Error: Expecting value: line 1 column 646 (char 645)\n",
      "Error decoding JSON for query: Show me any insights related to the interview with the victim's family yesterday.\n",
      "Error: Expecting ',' delimiter: line 1 column 152 (char 151)\n",
      "Error decoding JSON for query: Find all emails about 'meeting' sent between john@example.com and target phones last week\n",
      "Error: Expecting value: line 1 column 774 (char 773)\n",
      "Error decoding JSON for query: What calls from target phones to 0542164185 used UMTS protocol in the last 48 hours?\n",
      "Error: Expecting value: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: Find all CDRs of 0527893156 or 0523741852 with attachments in the past 48 hours\n",
      "Error: Expecting value: line 1 column 303 (char 302)\n",
      "Error decoding JSON for query: Show calls woth duartion of 30 minutes from target phones to numbers starting with '+1'.\n",
      "Error: Expecting value: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: List all communications initiated by phones with MSISDN +972541234567 to any target phone\n",
      "Error: Expecting value: line 1 column 380 (char 379)\n",
      "Error decoding JSON for query: Give me information about phones with alias name 'John's iPhone' or 'Mary's Samsung'\n",
      "Error: Expecting ',' delimiter: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: Show me any emails sent from jane@company.com to ariel042cohen@gmail.com with attachments in the past 2 weeks\n",
      "Error: Expecting value: line 1 column 678 (char 677)\n",
      "Error decoding JSON for query: Show me SMS messages exchanged from target phones to the number 0542164185 in the last 90 days\n",
      "Error: Expecting value: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: Show calls that had content from phones marked as suspicious to 0542164185.\n",
      "Error: Expecting value: line 1 column 232 (char 231)\n",
      "Error decoding JSON for query: Retrieve the details of suspicious phones with IMEI 356938035643809 or 490154203237518\n",
      "Error: Expecting value: line 1 column 150 (char 149)\n",
      "Error decoding JSON for query: Find all emails with 'secret' in the subject sent from spy@agency.gov to target phones\n",
      "Error: Expecting value: line 1 column 614 (char 613)\n",
      "Error decoding JSON for query: Show classified SMS conversations of 0527654321 in the first week of January 2024\n",
      "Error: Expecting value: line 1 column 239 (char 238)\n",
      "Error decoding JSON for query: Show me all calls made by target phones to +972501234567\n",
      "Error: Expecting value: line 1 column 487 (char 486)\n",
      "Error decoding JSON for query: Find all high priority visa requests submitted since 1 January 2024\n",
      "Error: Expecting value: line 1 column 150 (char 149)\n",
      "Error decoding JSON for query: Find all calls made using 3G technology from 0542164185 to target phones in the last month\n",
      "Error: Expecting value: line 1 column 749 (char 748)\n",
      "Error decoding JSON for query: List all classified emails with attachments sent to jane@company.com last month\n",
      "Error: Expecting value: line 1 column 398 (char 397)\n",
      "Error decoding JSON for query: Show me all sms sent from suspicious phones to the number 0542164185 in the last week.\n",
      "Error: Expecting value: line 1 column 232 (char 231)\n",
      "Error decoding JSON for query: What sms were sent from target phones to numbers starting with '052'?\n",
      "Error: Expecting value: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: Find all successful calls made from suspicious phones to 0542164185 in 2022.\n",
      "Error: Expecting value: line 1 column 698 (char 697)\n",
      "Error decoding JSON for query: List suspicious phones active within the last 2 weeks.\n",
      "Error: Expecting value: line 1 column 150 (char 149)\n",
      "Error decoding JSON for query: What calls 30 minutes long were made from suspicious phones to 0549112233?\n",
      "Error: Expecting value: line 1 column 232 (char 231)\n",
      "Error decoding JSON for query: Which suspicious phone had sent calls?\n",
      "Error: Expecting value: line 1 column 132 (char 131)\n",
      "Error decoding JSON for query: What calls were made from target phones using 3g or 4g?\n",
      "Error: Expecting value: line 1 column 333 (char 332)\n",
      "Error decoding JSON for query: Show me communication incoming records of the target phones with alias name starting with 'Agent'.\n",
      "Error: Expecting value: line 1 column 266 (char 265)\n",
      "Error decoding JSON for query: Show emails sent from devices marked as suspicious to mail ending with '@company.com'.\n",
      "Error: Expecting value: line 1 column 232 (char 231)\n",
      "Error decoding JSON for query: List the suspicious phones that communicated with +972559876543 in February 2024\n",
      "Error: Expecting value: line 1 column 132 (char 131)\n",
      "Error decoding JSON for query: Find all emails sent from johndoe@gmail.com using phones marked as suspicious\n",
      "Error: Expecting value: line 1 column 472 (char 471)\n",
      "Error decoding JSON for query: Find all calls to suspicious phones in the past 3 months.\n",
      "Error: Expecting value: line 1 column 495 (char 494)\n",
      "Error decoding JSON for query: Find all SMS messages containing the word 'meeting' sent from 0549876543 to target phones in the past week\n",
      "Error: Expecting value: line 1 column 751 (char 750)\n",
      "Error decoding JSON for query: Show me the suspicious phones that made calls to +1-555-987-6543.\n",
      "Error: Expecting value: line 1 column 132 (char 131)\n",
      "Error decoding JSON for query: Find emails with attachments sent from phones associated with alias name 'John' to johndoe@gmail.com.\n",
      "Error: Expecting value: line 1 column 258 (char 257)\n",
      "Error decoding JSON for query: Retrieve all emails with attachments sent from john@company.com since 1 March 2024.\n",
      "Error: Expecting value: line 1 column 359 (char 358)\n",
      "Error decoding JSON for query: Show me all emails with attachments sent to bob@company.com today\n",
      "Error: Expecting value: line 1 column 539 (char 538)\n",
      "Error decoding JSON for query: Show me all emails with PDF attachments sent in the first half of October 2024.\n",
      "Error: Expecting value: line 1 column 377 (char 376)\n",
      "Error decoding JSON for query: Show me all classified communications in the past 30 days\n",
      "Error: Expecting value: line 1 column 135 (char 134)\n",
      "Error decoding JSON for query: Show me sms sent from target phones to  0501234567\n",
      "Error: Expecting value: line 1 column 483 (char 482)\n",
      "Error decoding JSON for query: Show emails to dani@gmail.com that have PDF attachments.\n",
      "Error: Expecting value: line 1 column 273 (char 272)\n",
      "Error decoding JSON for query: Which phones have the alias name 'John's work phone'?\n",
      "Error: Expecting ',' delimiter: line 1 column 153 (char 152)\n",
      "Error decoding JSON for query: Find all suspicious phones\n",
      "Error: Expecting value: line 1 column 150 (char 149)\n",
      "Error decoding JSON for query: Show me sms sent from a target phone to 0542164185\n",
      "Error: Expecting value: line 1 column 483 (char 482)\n",
      "Error decoding JSON for query: Display voice calls lasting 30 minutes to non-target phones\n",
      "Error: Expecting value: line 1 column 437 (char 436)\n",
      "Error decoding JSON for query: Show sms sent on 2023-02-14 from phones with imei 123456789012345 to target phones\n",
      "Error: Expecting value: line 1 column 558 (char 557)\n",
      "Error decoding JSON for query: Find SMS messages containing the word 'meeting' sent from target phones to the number 0526357106.\n",
      "Error: Expecting value: line 1 column 228 (char 227)\n",
      "Error decoding JSON for query: What SMS messages were sent from suspicious phones to 0549876543 containing the word 'urgent'?\n",
      "Error: Expecting value: line 1 column 598 (char 597)\n",
      "Error decoding JSON for query: Show all phones labeled as suspicious.\n",
      "Error: Expecting value: line 1 column 150 (char 149)\n",
      "Error decoding JSON for query: List emails with attachments sent from ariel042cohen@gmail.com.\n",
      "Error: Expecting value: line 1 column 281 (char 280)\n"
     ]
    }
   ],
   "source": [
    "# prepare our data for training. we combine our user query with field description\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    # Remove any leading/trailing whitespace\n",
    "    json_string = json_string.strip()\n",
    "    \n",
    "    # Ensure the string is enclosed in curly braces\n",
    "    if not json_string.startswith('{'):\n",
    "        json_string = '{' + json_string\n",
    "    if not json_string.endswith('}'):\n",
    "        json_string = json_string + '}'\n",
    "    \n",
    "    # Replace single quotes with double quotes, but not within values\n",
    "    json_string = re.sub(r\"(?<!\\\\)'\", '\"', json_string)\n",
    "    \n",
    "    # Remove any trailing commas before closing braces or brackets\n",
    "    json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "    \n",
    "    return json_string\n",
    "\n",
    "def prepare_data_for_training(user_query, fields_mapping):\n",
    "    inputs, labels = [], []\n",
    "\n",
    "    for _, row in user_queries.iterrows():\n",
    "        query = row['question']\n",
    "        cleaned_json_string = clean_json_string(row['json'])\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(cleaned_json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for query: {query}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            continue  # Skip this row and continue with the next one\n",
    "        \n",
    "        # extract entity types and relation target types\n",
    "        entity_type = json_data.get('entityType', '')\n",
    "        relation_type = json_data.get('relationTargetType', '')\n",
    "\n",
    "        # get the description for each entity type\n",
    "        fields = fields_mapping.get(entity_type, [])\n",
    "        field_descriptions = ';'.join([f\"{field['field_name']}: {field['description']}\" for field in fields])\n",
    "\n",
    "        #combine query with descriptions\n",
    "        input_text = f\"Query: {query}. Entity: {entity_type}. Fields: {field_descriptions}\"\n",
    "        inputs.append(input_text)\n",
    "        labels.append(entity_type if not relation_type else f\"{entity_type}|{relation_type}\")\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "# prepare data\n",
    "inputs, labels = prepare_data_for_training(user_queries, entity_to_field_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 1: Query: Find all calls made using 3G technology.. Entity: CDR. Fields: ifc.ootb.CDR.callStatus: Status of the call: \"Successful\", \"Failed\", \"Blocked\", or \"Redirected\";ifc.CDR.caseCode: Unique code identifying a specific case;ifc.CDR.chatTopic: Topic or subject of discussion in the chat;ifc.ootb.CDR.createDateTime: Date and time of record creation.;ifc.ootb.CDR.direction: Direction of the call (incoming, outgoing);ifc.ootb.CDR.duration: Duration of the communication in minutes. You can ask it For example: 1min -> 60;ifc.CDR.emailSubject: Subject of the email communication;ifc.ootb.CDR.endTime: Time when the communication ended;ifc.ootb.CDR.hasContent: Indicates if the communication has content;ifc.ootb.CDR.imei: this field is intended to store the IMEI number of the device who made a call.;ifc.ootb.CDR.imei2: this field is intended to store the IMEI number of the device that is receiving the call. It serves the same purposes as the caller's IMEI but for the receiving side of the communication.;ifc.ootb.CDR.imsi: The IMSI that initiate the call as a sender. IMSI stands for International Mobile Subscriber Identity. It is a unique number associated with all cellular networks. It is used to identify the user of a cellular network and is a key part of any mobile network. The IMSI is stored on a SIM card and consists of up to 15 digits.The structure of the IMSI is as follows:Mobile Country Code (MCC): The first three digits represent the country code, identifying the country in which the carrier operates.Mobile Network Code (MNC): The next two or three digits identify the mobile network within the country.Mobile Subscriber Identification Number (MSIN): The remainder of the IMSI is a unique identifier for the subscriber within the network.;ifc.ootb.CDR.imsi2: The IMSI that received the call as a receiver. IMSI is an International Mobile Subscriber Identity (IMSI) is a 15-digit number for every user in a Global System for Mobile communication (GSM). The IMSI is used by Mobile Network Operators (MNOs) and is an important part of the Subscriber Identity Module (SIM) profile.;ifc.CDR.interceptionCriteria: Criteria for intercepting the communication;ifc.CDR.interceptionFilter: Filter applied for interception;ifc.ootb.CDR.ipAddress: IP address of the communication's  Origin ;ifc.ootb.CDR.isAttachment: Indicates if the communication has an attachment;ifc.CDR.isClassified: Indicates if the communication is classified;ifc.ootb.CDR.msisdn: MSISDN or phone number is the phone number associated with a mobile network subscriber as a sender. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to make calls, send SMS, and utilize mobile data as sender.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).;ifc.ootb.CDR.msisdn2: MSISDN or phone number is the phone number associated with a mobile network subscriber as a receiver. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to receive calls, receive SMS, and utilize mobile data as reciever.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).;ifc.ootb.CDR.protocol: Communication protocol used like GSM or UMTS;ifc.ootb.CDR.provider: Provider of the network service;ifc.ootb.CDR.smsProtocol: Protocol used for SMS communication;ifc.ootb.CDR.smsText: Text content of the SMS;ifc.ootb.CDR.smsType: Type of the SMS (e.g., text, binary);ifc.ootb.CDR.startTime: Time when the communication started;ifc.CDR.subcaseCode: Code for the subcase of a known case. Identify between this field and the ifc.CDR.caseCode;ifc.CDR.targetCode: Code identifying the target of the communication;ifc.ootb.CDR.technology: Technology used in the communication, either: 2G,3G,4G,5G,other, WiFi;ifc.ootb.CDR.type: Type of the communication, either: None-Call, Text,Voice, VoiceEdited, Web or Email. Text type is SMS, Voice type is Phone Calls;ifc.ootb.CDR.mainLanguage: Primary language of the communication content;ifc.ootb.CDR.ipAddress2: IP address of the communication's  Destination;ifc.ootb.CDR.originatorEmailAddress: Email address of the sender;ifc.ootb.CDR.destinationEmailAddress: Email address of the recipient;ifc.ootb.CDR.orignatorNetworkName: Name of the originator's network;ifc.ootb.CDR.destinatorNetworkName: Name of the recipient's network\n",
      "Label 1: CDR\n",
      "Input 2: Query: List all Reddit comments posted yesterday with a negative sentiment. Entity: Web Activity. Fields: ifc.ootb.Orbis.platform: the social network name form which the actor was collected (e.g. facebook instagram linkedin etc. );ifc.ootb.WebActivity.writerFullName: Full name of the writer who wrote the post;ifc.ootb.WebActivity.writerUri: URI of the writers profile or content;ifc.ootb.Orbis.updateDate: Date when the post was updated;ifc.ootb.Orbis.collectDate: Date when the post data was collected;ifc.ootb.Orbis.publishDate: Date when the post was published or created;ifc.ootb.WebActivity.numberOfLikes: Number of likes on the activity;ifc.ootb.WebActivity.numberShares: Number of times the activity was shared;ifc.ootb.WebActivity.numberActivities: Count of all web activities (e.g. posts, reshares) nested under this web activity;ifc.ootb.WebActivity.numberTags: Number of tags associated with the activity;ifc.ootb.WebActivity.mainLanguage: Main language of the activity;ifc.ootb.WebActivity.sentiment: sentiment of the text with the web activity (can be positive or negative);ifc.ootb.WebActivity.sentimentScore: a score between 0 (low) and 1 (high) of the sentiment analysis certainty of the text of the activity;ifc.ootb.WebActivity.text: Text content of the web activity;ifc.ootb.Orbis.creditCard: NER extraction of entities representing credit card information as extracted from ORBIS;ifc.ootb.Orbis.email: NER extraction of entities representing emails as extracted from ORBIS;ifc.ootb.Orbis.hashtags: Hashtags mentioned in the activity text;ifc.ootb.Orbis.ipAddress: IP address mentioned in the activity text;ifc.ootb.Orbis.mentions: a mention of a profile in social media usually with an @ sign;ifc.ootb.Orbis.nationality: Nationality mentioned in the activity text;ifc.ootb.Orbis.phone: Phone number mentioned in the activity text;ifc.ootb.Orbis.religion: Religions mentioned in the activity text;ifc.ootb.Orbis.url: URL mentioned in the activity text;ifc.ootb.Orbis.landmarks: names of landmarks found in the activity (e.g eiffel tower);ifc.ootb.WebActivity.title: Title of the web activity\n",
      "Label 2: Web Activity\n",
      "Input 3: Query: Show me investigations that are either open or were created in the last 3 months. Entity: Investigation. Fields: ifc.core.description: Detailed summary of the investigation;ifc.core.dueTime: Scheduled completion time for the investigation;ifc.core.state: Current state of the investigation. either \"open\" or \"close\";ifc.core.priority: Priority level of the investigation: \"regular\", \"medium\" or \"high\" in ascending order;ifc.core.createtime: Creation time of the investigation record\n",
      "Label 3: Investigation\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Input {i+1}: {inputs[i]}\")\n",
    "    print(f\"Label {i+1}: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 553, Validation size: 139\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    inputs, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training size: {len(train_inputs)}, Validation size: {len(val_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"twmkn9/albert-base-v2-squad2\"\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "train_encodings = tokenizer(train_inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# labels to tensors. we can't have names only machine values\n",
    "# remove duplicates and iterate through to assign a number\n",
    "unique_labels = list(set(labels))\n",
    "train_labels_tensor = torch.tensor([unique_labels.index(lbl) for lbl in train_labels])\n",
    "val_labels_tensor = torch.tensor([unique_labels.index(lbl) for lbl in val_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training encodings: torch.Size([553, 512]), Labels: torch.Size([553])\n",
      "Validation encodings: torch.Size([139, 512]), Labels: torch.Size([139])\n"
     ]
    }
   ],
   "source": [
    "# do our dimensions match?\n",
    "print(f\"Training encodings: {train_encodings['input_ids'].shape}, Labels: {train_labels_tensor.shape}\")\n",
    "print(f\"Validation encodings: {val_encodings['input_ids'].shape}, Labels: {val_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([    2, 25597,    45,   298,    55,    65,  3029,    29,    21,  9403,\n",
      "           16,   712,   902,     9,     9,  9252,    45,  1745,   139,     9,\n",
      "         2861,    45,   100,   150,     9,  4328, 11872,     9,   150,  3807,\n",
      "            9,  9200, 10631,   267,    45,  1782,    16,    14,   645,    45,\n",
      "           13,     7, 29245,  1566,     7,    15,    13,     7, 24910,    69,\n",
      "            7,    15,    13,     7, 12048,    69,     7,    15,    54,    13,\n",
      "            7,    99, 14147,     7,    73,   821,   150,     9,   150,  3807,\n",
      "            9, 10325,  9375,    45,  2619,  1797, 13785,    21,  1903,   610,\n",
      "           73,   821,   150,     9,   150,  3807,     9, 13409,  3880,   596,\n",
      "           45,  8303,    54,  1550,    16,  5460,    19,    14,  6615,    73,\n",
      "          821,   150,     9,  4328, 11872,     9,   150,  3807,     9, 18475,\n",
      "         1373,   891,    45,  1231,    17,    85,    16,   571,  2502,     9,\n",
      "           73,   821,   150,     9,  4328, 11872,     9,   150,  3807,     9,\n",
      "        14706,   872,    45,  1400,    16,    14,   645,    13,     5,   108,\n",
      "         9076,    15, 25193,     6,    73,   821,   150,     9,  4328, 11872,\n",
      "            9,   150,  3807,     9, 10852,   857,    45,  9403,    16,    14,\n",
      "         3291,    19,   902,     9,    42,    92,  1349,    32,    26,   823,\n",
      "           45,   137,  2160,    13,     8,     1,  2083,    73,   821,   150,\n",
      "            9,   150,  3807,     9,    62,  8079, 28743,    45,  1550,    16,\n",
      "           14,  8517,  3291,    73,   821,   150,     9,  4328, 11872,     9,\n",
      "          150,  3807,     9,  2451,   891,    45,    85,    76,    14,  3291,\n",
      "         1272,    73,   821,   150,     9,  4328, 11872,     9,   150,  3807,\n",
      "            9,  6482, 25424,    45,  6475,   100,    14,  3291,    63,  2331,\n",
      "           73,   821,   150,     9,  4328, 11872,     9,   150,  3807,     9,\n",
      "           49, 14001,    45,    48,   575,    25,  2081,    20,  1718,    14,\n",
      "           31, 14001,   234,    16,    14,  3646,    72,   117,    21,   645,\n",
      "            9,    73,   821,   150,     9,  4328, 11872,     9,   150,  3807,\n",
      "            9,    49, 14001,   135,    45,    48,   575,    25,  2081,    20,\n",
      "         1718,    14,    31, 14001,   234,    16,    14,  3646,    30,    25,\n",
      "         3396,    14,   645,     9,    32,  2589,    14,   205,  4612,    28,\n",
      "           14, 21326,    22,    18,    31, 14001,    47,    26,    14,  3396,\n",
      "          270,    16,    14,  3291,     9,    73,   821,   150,     9,  4328,\n",
      "        11872,     9,   150,  3807,     9,  1660,    18,    49,    45,    14,\n",
      "          797,    18,    49,    30, 17014,    14,   645,    28,    21,  2660,\n",
      "          106,     9,   797,    18,    49,  3486,    26,   294,  3241,    13,\n",
      "        20330,   139,  3270,     9,    32,    25,    21,  2619,   234,  1598,\n",
      "           29,    65, 14844,  5540,     9,    32,    25,   147,    20,  5808,\n",
      "           14,  4155,    16,    21, 14844,   982,    17,    25,    21,  1246,\n",
      "          141,    16,   186,  3241,   982,     9,    14,   797,    18,    49,\n",
      "           25,  8214,    27,    21,  4861,  2056,    17,  2043,    16,    71,\n",
      "           20,   357, 19076,     9,   124,  1411,    16,    14,   797,    18,\n",
      "           49,    25,    28,  2415,    45, 12571,   475,  1797,    13,     5,\n",
      "           79,  3384,     6,    45,    14,    64,   132, 19076,  3501,    14,\n",
      "          475,  1797,    15, 13785,    14,   475,    19,    56,    14,  5812,\n",
      "         4554,     9, 12571,   982,  1797,    13,     5,    79,  6897,     6,\n",
      "           45,    14,   328,    81,    54,   132, 19076,  5808,    14,  3241,\n",
      "          982,   363,    14,   475,     9, 12571,    13, 20330,   139,  8575,\n",
      "          234,    13,     5,    79,    18,   108,     6,    45,    14,  5874,\n",
      "           16,    14,   797,    18,    49,    25,    21,  2619, 25570,    26,\n",
      "           14,    13, 20330,   139,   363,    14,   982,     9,    73,   821,\n",
      "          150,     9,  4328, 11872,     9,   150,  3807,     9,  1660,    18,\n",
      "           49,     3]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects for training and validation\n",
    "train_dataset = EntityDataset(train_encodings, train_labels_tensor)\n",
    "val_dataset = EntityDataset(val_encodings, val_labels_tensor)\n",
    "\n",
    "# check the first sample from the training dataset\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare results folder\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a unique output directory\n",
    "base_output_dir = \"./results\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(base_output_dir, f\"run_{current_time}\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving training dataset\n",
      "Training new model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at twmkn9/albert-base-v2-squad2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/207 [03:32<?, ?it/s]\n",
      "                                                 \n",
      "100%|| 207/207 [48:01<00:00, 13.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2881.6198, 'train_samples_per_second': 0.576, 'train_steps_per_second': 0.072, 'train_loss': 0.4922458814538043, 'epoch': 2.99}\n",
      "AlbertForSequenceClassification(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertSdpaAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (pooler_activation): Tanh()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model and training run\n",
    "# check for local first\n",
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# output_dir = \"./results/run_20241014_163059\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Assuming Training is already done and we have models locally or configuration vars\n",
    "if os.path.exists(output_dir) and os.path.exists(output_dir + \"/spiece.model\"):\n",
    "    print(f\"Loading model from {output_dir}\")\n",
    "    model = AlbertForSequenceClassification.from_pretrained(output_dir)\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "    print(f\"Model loaded from {output_dir}\")\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Reload training dataset\n",
    "    with open(\"train_dataset.pkl\", \"rb\") as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "\n",
    "    # Reload validation dataset\n",
    "    with open(\"val_dataset.pkl\", \"rb\") as f:\n",
    "        val_dataset = pickle.load(f)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "else:\n",
    "    print('saving training dataset')\n",
    "    # Save training dataset\n",
    "    with open(f\"{output_dir}/train_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "\n",
    "    # Save validation dataset\n",
    "    with open(f\"{output_dir}/val_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(val_dataset, f)\n",
    "\n",
    "    print(f\"Training new model\")\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=len(unique_labels))\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Labels: ['CDR', 'Investigation', 'Insight', 'Phone', 'Report', 'Web Actor', 'Person', 'EVisa Request', 'Web Activity']\n",
      "Number of Classes (num_labels): 9\n",
      "Label IDs: tensor([0, 8, 1, 2, 8, 3, 8, 2, 3, 4, 4, 0, 8, 0, 8, 6, 3, 4, 4, 1, 4, 6, 0, 1,\n",
      "        5, 4, 6, 3, 0, 7, 0, 8, 5, 0, 6, 0, 6, 2, 2, 5, 2, 4, 3, 8, 6, 5, 0, 2,\n",
      "        3, 1, 0, 1, 1, 0, 0, 4, 4, 2, 5, 2, 8, 3, 0, 7, 8, 1, 0, 4, 3, 0, 8, 0,\n",
      "        0, 0, 5, 6, 0, 1, 1, 0, 0, 4, 8, 8, 8, 5, 8, 5, 0, 5, 3, 8, 3, 4, 0, 1,\n",
      "        1, 0, 3, 2, 8, 8, 0, 1, 6, 3, 8, 8, 4, 0, 0, 7, 5, 8, 7, 0, 4, 1, 6, 8,\n",
      "        3, 5, 0, 3, 5, 6, 8, 8, 3, 2, 8, 2, 6, 8, 6, 8, 8, 0, 0, 8, 0, 0, 5, 5,\n",
      "        5, 8, 4, 6, 4, 3, 0, 0, 4, 8, 8, 0, 5, 0, 8, 1, 8, 6, 0, 5, 5, 2, 0, 0,\n",
      "        8, 0, 6, 4, 8, 8, 0, 0, 3, 1, 1, 8, 3, 5, 5, 1, 3, 1, 5, 2, 0, 1, 0, 0,\n",
      "        4, 7, 3, 4, 0, 8, 2, 0, 3, 3, 0, 0, 0, 0, 8, 0, 4, 8, 6, 8, 4, 5, 6, 2,\n",
      "        8, 1, 8, 0, 4, 2, 5, 6, 8, 1, 1, 6, 4, 8, 7, 0, 1, 0, 0, 0, 0, 5, 5, 4,\n",
      "        3, 0, 3, 5, 0, 6, 0, 1, 4, 0, 6, 8, 6, 3, 2, 2, 0, 0, 4, 2, 0, 4, 8, 5,\n",
      "        5, 2, 0, 1, 8, 3, 2, 1, 1, 2, 4, 3, 4, 0, 0, 3, 0, 4, 6, 8, 4, 3, 6, 6,\n",
      "        1, 6, 0, 5, 2, 0, 3, 4, 3, 3, 2, 3, 0, 2, 8, 1, 6, 0, 2, 0, 0, 1, 0, 2,\n",
      "        1, 8, 5, 4, 0, 0, 3, 0, 5, 0, 4, 0, 1, 5, 8, 8, 1, 3, 4, 3, 5, 5, 5, 4,\n",
      "        8, 3, 0, 0, 2, 0, 8, 2, 8, 0, 6, 0, 0, 0, 0, 0, 1, 6, 0, 6, 3, 0, 0, 3,\n",
      "        4, 2, 6, 8, 0, 4, 0, 3, 0, 4, 0, 0, 5, 3, 1, 0, 0, 2, 4, 8, 8, 0, 0, 3,\n",
      "        8, 6, 1, 2, 0, 0, 5, 1, 2, 5, 8, 6, 0, 4, 0, 3, 0, 2, 6, 5, 0, 0, 8, 4,\n",
      "        8, 0, 4, 6, 3, 0, 4, 1, 0, 1, 6, 1, 0, 2, 8, 5, 8, 5, 6, 1, 0, 0, 2, 5,\n",
      "        0, 3, 0, 1, 8, 8, 2, 1, 6, 2, 5, 1, 3, 2, 0, 0, 3, 3, 6, 4, 5, 3, 1, 8,\n",
      "        5, 0, 0, 0, 8, 0, 1, 0, 0, 0, 4, 6, 1, 4, 1, 8, 3, 6, 3, 0, 0, 0, 6, 2,\n",
      "        0, 0, 8, 4, 1, 8, 3, 0, 4, 2, 4, 0, 8, 4, 3, 0, 4, 5, 1, 1, 0, 5, 4, 0,\n",
      "        8, 0, 1, 0, 6, 4, 8, 1, 6, 3, 3, 1, 5, 8, 3, 4, 8, 3, 1, 4, 4, 0, 1, 8,\n",
      "        8, 8, 6, 3, 3, 4, 1, 4, 5, 5, 0, 1, 0, 2, 0, 7, 3, 4, 1, 0, 4, 6, 2, 0,\n",
      "        5, 5, 4, 0, 3, 5, 1, 0, 4, 2, 5, 1, 8, 1, 2, 0, 5, 4, 8, 0, 3, 8, 0, 6,\n",
      "        2, 3, 4, 0, 5, 0, 0, 1, 3, 6, 0, 4, 0, 0, 4, 8, 2, 0, 8, 0, 5, 0, 0, 0,\n",
      "        0, 4, 3, 5, 8, 3, 0, 0, 5, 0, 0, 8, 0, 6, 0, 5, 1, 5, 6, 0, 8, 1, 3, 8,\n",
      "        0, 8, 8, 1, 3, 8, 8, 1, 3, 5, 8, 2, 0, 5, 2, 8, 8, 3, 0, 2, 0, 1, 2, 7,\n",
      "        3, 1, 0, 5, 8, 0, 2, 5, 0, 5, 0, 4, 2, 8, 0, 1, 3, 3, 3, 1, 8, 4, 0, 6,\n",
      "        5, 5, 8, 0, 0, 3, 0, 6, 5, 1, 8, 2, 3, 5, 0, 0, 0, 1, 3, 3])\n",
      "Max Label ID: 8, Expected: 8\n"
     ]
    }
   ],
   "source": [
    "# Check the number of unique labels in your dataset\n",
    "print(f\"Unique Labels: {unique_labels}\")\n",
    "print(f\"Number of Classes (num_labels): {len(unique_labels)}\")\n",
    "# Convert labels to integers from 0 to len(unique_labels) - 1\n",
    "label_ids = torch.tensor([unique_labels.index(lbl) for lbl in labels])\n",
    "\n",
    "# Verify label IDs are within range\n",
    "print(f\"Label IDs: {label_ids}\")\n",
    "print(f\"Max Label ID: {label_ids.max()}, Expected: {len(unique_labels) - 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trainer that's ok. We just won't save\n"
     ]
    }
   ],
   "source": [
    "should_save = False\n",
    "if should_save:\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "else:\n",
    "    print(f\"No Trainer that's ok. We just won't save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_model(trainer, val_dataset, unique_labels):\n",
    "    # get predictions\n",
    "    preds = trainer.predict(val_dataset)\n",
    "\n",
    "    # covert ML output to labels\n",
    "    preds_labels = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "\n",
    "    # extract TRUE labels\n",
    "    true_labels = [val_dataset[i]['labels'].item() for i in range(len(val_dataset))]\n",
    "\n",
    "    #compute F1 score\n",
    "    f1 = f1_score(true_labels, preds_labels, average='weighted')\n",
    "\n",
    "    # print report\n",
    "    print(f\"weighted f1 score: {f1}\")\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(true_labels, preds_labels, target_names=unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [01:44,  1.36s/it]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score: 1.0\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          CDR       1.00      1.00      1.00        37\n",
      "Investigation       1.00      1.00      1.00        15\n",
      "      Insight       1.00      1.00      1.00        11\n",
      "        Phone       1.00      1.00      1.00        16\n",
      "       Report       1.00      1.00      1.00        14\n",
      "    Web Actor       1.00      1.00      1.00        14\n",
      "       Person       1.00      1.00      1.00        10\n",
      "EVisa Request       1.00      1.00      1.00         2\n",
      " Web Activity       1.00      1.00      1.00        20\n",
      "\n",
      "     accuracy                           1.00       139\n",
      "    macro avg       1.00      1.00      1.00       139\n",
      " weighted avg       1.00      1.00      1.00       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(trainer, val_dataset, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on trained model\n",
    "def infer(model, tokenizer, query, unique_labels):\n",
    "    # Tokenize the input query\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted label ID\n",
    "    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    # Convert the label ID back to the original label name\n",
    "    predicted_label = unique_labels[predicted_label_id]\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Entity for Query: Where are the most fraudulant transactions taking place?\n",
      "Predicted Entity Type: Insight\n"
     ]
    }
   ],
   "source": [
    "# Test the inference function with a sample query\n",
    "sample_query = \"Where are the most fraudulant transactions taking place?\"\n",
    "predicted_entity = infer(model, tokenizer, sample_query, unique_labels)\n",
    "\n",
    "print(f\"Predicted Entity for Query: {sample_query}\")\n",
    "print(f\"Predicted Entity Type: {predicted_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
