{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: sentencepiece in ./env/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./env/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./env/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./env/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./env/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./env/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./env/lib/python3.11/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !python -m venv env\n",
    "# !source env/bin/activate  \n",
    "!pip install torch transformers scikit-learn pandas sentencepiece\n",
    "!pip install numpy==1.26.4 --force-reinstall\n",
    "# needed for training\n",
    "! pip install -U accelerate\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "<a href=\"images/processing-step.png\" target=\"_blank\"> <img src=\"images/processing-step.png\" alt=\"High-level overview of the Preprocessing Step\" style=\"max-width: 740px;\" /></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find all calls made using 3G technology.</td>\n",
       "      <td>{'entityType': 'CDR', 'statements': [{'type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List all Reddit comments posted yesterday with...</td>\n",
       "      <td>{'entityType': 'Web Activity', 'statements': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Show me investigations that are either open or...</td>\n",
       "      <td>{'entityType': 'Investigation', 'statements': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find all insights related to the witness Jane ...</td>\n",
       "      <td>{'entityType': 'Insight', 'statements': [{'typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List all web activities updated in the last da...</td>\n",
       "      <td>{'entityType': 'Web Activity', 'statements': [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           Find all calls made using 3G technology.   \n",
       "1  List all Reddit comments posted yesterday with...   \n",
       "2  Show me investigations that are either open or...   \n",
       "3  Find all insights related to the witness Jane ...   \n",
       "4  List all web activities updated in the last da...   \n",
       "\n",
       "                                                json  \n",
       "0  {'entityType': 'CDR', 'statements': [{'type': ...  \n",
       "1  {'entityType': 'Web Activity', 'statements': [...  \n",
       "2  {'entityType': 'Investigation', 'statements': ...  \n",
       "3  {'entityType': 'Insight', 'statements': [{'typ...  \n",
       "4  {'entityType': 'Web Activity', 'statements': [...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and create mapping into new dataframe\n",
    "# right now we are just using the user query \n",
    "### TODO files are static paths now.  we need to make them dynamic and maybe add a nice UI to select the file\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "user_queries = pd.read_csv('user_queries.csv')\n",
    "user_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>744</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>742</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Show me insights where the text includes 'witn...</td>\n",
       "      <td>{'entityType': 'Phone', 'statements': [{'type'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "count                                                 744   \n",
       "unique                                                742   \n",
       "top     Show me insights where the text includes 'witn...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                     json  \n",
       "count                                                 744  \n",
       "unique                                                721  \n",
       "top     {'entityType': 'Phone', 'statements': [{'type'...  \n",
       "freq                                                    3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_queries.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare our data for training. we combine our user query with field description\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    # Remove any leading/trailing whitespace\n",
    "    json_string = json_string.strip()\n",
    "    \n",
    "    # Ensure the string is enclosed in curly braces\n",
    "    if not json_string.startswith('{'):\n",
    "        json_string = '{' + json_string\n",
    "    if not json_string.endswith('}'):\n",
    "        json_string = json_string + '}'\n",
    "    \n",
    "    # Replace single quotes with double quotes, but not within values\n",
    "    json_string = re.sub(r\"(?<!\\\\)'\", '\"', json_string)\n",
    "    \n",
    "    # Remove any trailing commas before closing braces or brackets\n",
    "    json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "    \n",
    "    return json.loads(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find all calls made using 3G technology.</td>\n",
       "      <td>CDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List all Reddit comments posted yesterday with...</td>\n",
       "      <td>Web Activity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Show me investigations that are either open or...</td>\n",
       "      <td>Investigation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find all insights related to the witness Jane ...</td>\n",
       "      <td>Insight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List all web activities updated in the last da...</td>\n",
       "      <td>Web Activity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question          label\n",
       "0           Find all calls made using 3G technology.            CDR\n",
       "1  List all Reddit comments posted yesterday with...   Web Activity\n",
       "2  Show me investigations that are either open or...  Investigation\n",
       "3  Find all insights related to the witness Jane ...        Insight\n",
       "4  List all web activities updated in the last da...   Web Activity"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def extract_label_from_json(json_str):\n",
    "    \"\"\"\n",
    "    Extracts the 'entityType' and 'relationTargetType' from the given JSON string.\n",
    "    If 'relationTargetType' is missing, the label will only include 'entityType'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = clean_json_string(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        # print(f\"Error decoding JSON for query: {json_str}\")\n",
    "        # print(f\"Error: {e}\")\n",
    "        json_data = {}\n",
    "\n",
    "    entity_type = json_data.get('entityType', '')\n",
    "    relation_type = json_data.get('statements', [{}])[0].get('parameters', {}).get('relationTargetType', [''])[0]\n",
    "\n",
    "    # Combine entity and relation types into a single label\n",
    "    label = entity_type if not relation_type else f\"{entity_type}|{relation_type}\"\n",
    "    return label\n",
    "\n",
    "# Apply the function to extract labels from the JSON column\n",
    "user_queries['label'] = user_queries['json'].apply(extract_label_from_json)\n",
    "\n",
    "# Inspect the DataFrame to ensure labels are extracted correctly\n",
    "user_queries[['question', 'label']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {0: 'CDR', 1: 'Web Activity', 2: 'Investigation', 3: 'Insight', 4: 'Phone', 5: '', 6: 'Report', 7: 'Person', 8: 'Web Actor', 9: 'EVisa Request', 10: 'Web Activity|Web Actor', 11: 'CDR|Phone', 12: 'Phone|CDR'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CDR</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Web Activity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Investigation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Insight</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Web Activity</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           label  label_numeric\n",
       "0            CDR              0\n",
       "1   Web Activity              1\n",
       "2  Investigation              2\n",
       "3        Insight              3\n",
       "4   Web Activity              1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the labels into numeric format\n",
    "user_queries['label_numeric'], label_mapping = pd.factorize(user_queries['label'])\n",
    "\n",
    "# Store the label mapping for future reference (useful during inference)\n",
    "print(\"Label Mapping:\", dict(enumerate(label_mapping)))\n",
    "\n",
    "# Inspect the encoded labels\n",
    "user_queries[['label', 'label_numeric']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([744, 128])\n",
      "Attention Mask shape: torch.Size([744, 128])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "import torch\n",
    "\n",
    "# Initialize the ALBERT tokenizer\n",
    "tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
    "\n",
    "def tokenize_input(query):\n",
    "    \"\"\"\n",
    "    Tokenizes the given input query using the ALBERT tokenizer.\n",
    "    Returns input_ids and attention_mask as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        query,\n",
    "        truncation=True,        # Truncate sequences longer than max length\n",
    "        padding='max_length',   # Pad shorter sequences to the max length\n",
    "        max_length=128,         # Set a consistent sequence length\n",
    "        return_tensors=\"pt\"     # Return as PyTorch tensors\n",
    "    )\n",
    "\n",
    "# Tokenize all input queries\n",
    "tokenized_data = user_queries['question'].apply(tokenize_input)\n",
    "\n",
    "# Convert the tokenized data into input_ids and attention_masks\n",
    "input_ids = torch.cat([x['input_ids'] for x in tokenized_data], dim=0)\n",
    "attention_masks = torch.cat([x['attention_mask'] for x in tokenized_data], dim=0)\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention Mask shape: {attention_masks.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 595\n",
      "Validation set size: 74\n",
      "Test set size: 75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract labels as tensors\n",
    "labels = torch.tensor(user_queries['label_numeric'].values)\n",
    "\n",
    "# Split the data into train, validation, and test sets (80/10/10 split)\n",
    "train_inputs, temp_inputs, train_labels, temp_labels = train_test_split(\n",
    "    input_ids, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "val_inputs, test_inputs, val_labels, test_labels = train_test_split(\n",
    "    temp_inputs, temp_labels, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {train_inputs.shape[0]}\")\n",
    "print(f\"Validation set size: {val_inputs.shape[0]}\")\n",
    "print(f\"Test set size: {test_inputs.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "<a href=\"images/training-step.png\" target=\"_blank\"> <img src=\"images/training-step.png\" alt=\"High-level overview of the Training Step\" style=\"max-width: 740px;\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class QueryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom Dataset class to handle the input_ids, attention_masks, and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Total number of samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the inputs, masks, and labels by index\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_masks[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create Dataset instances for train, validation, and test sets\n",
    "train_dataset = QueryDataset(train_inputs, train_inputs, train_labels)\n",
    "val_dataset = QueryDataset(val_inputs, val_inputs, val_labels)\n",
    "test_dataset = QueryDataset(test_inputs, test_inputs, test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AlbertForSequenceClassification(\n",
       "  (albert): AlbertModel(\n",
       "    (embeddings): AlbertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "    (encoder): AlbertTransformer(\n",
       "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
       "      (albert_layer_groups): ModuleList(\n",
       "        (0): AlbertLayerGroup(\n",
       "          (albert_layers): ModuleList(\n",
       "            (0): AlbertLayer(\n",
       "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (attention): AlbertSdpaAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (attention_dropout): Dropout(p=0, inplace=False)\n",
       "                (output_dropout): Dropout(p=0, inplace=False)\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              )\n",
       "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (activation): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (pooler_activation): Tanh()\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AlbertForSequenceClassification, AlbertConfig\n",
    "\n",
    "# Get the number of unique labels\n",
    "num_labels = len(torch.unique(train_labels))\n",
    "\n",
    "# Create a configuration object with the correct number of labels\n",
    "config = AlbertConfig.from_pretrained(\"albert-base-v2\", num_labels=num_labels)\n",
    "\n",
    "# Load the ALBERT model for sequence classification\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", config=config)\n",
    "\n",
    "# Move the model to the appropriate device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare results folder\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a unique output directory\n",
    "base_output_dir = \"./results\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(base_output_dir, f\"run_{current_time}\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/transformers/training_args.py:1560: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01, # Weight decay to reduce overfitting\n",
    "    no_cuda=True,           \n",
    "    logging_dir=\"./logs\"   \n",
    ")\n",
    "\n",
    "# Initialize the Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                       # ALBERT model to be trained\n",
    "    args=training_args,                # Training arguments\n",
    "    train_dataset=train_dataset,       # Training dataset\n",
    "    eval_dataset=val_dataset           # Validation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 222/222 [13:31<00:00,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 811.9126, 'train_samples_per_second': 2.199, 'train_steps_per_second': 0.273, 'train_loss': 2.238061440957559, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=222, training_loss=2.238061440957559, metrics={'train_runtime': 811.9126, 'train_samples_per_second': 2.199, 'train_steps_per_second': 0.273, 'total_flos': 10610332942848.0, 'train_loss': 2.238061440957559, 'epoch': 2.9798657718120807})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer saved to ./results/run_20241015_153838\n"
     ]
    }
   ],
   "source": [
    "should_save = False\n",
    "if should_save:\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "else:\n",
    "    print(f\"No Trainer that's ok. We just won't save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:13<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2667\n",
      "F1-Score (Weighted): 0.1716\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.70      0.53        20\n",
      "           1       0.14      0.60      0.23        10\n",
      "           2       0.00      0.00      0.00         7\n",
      "           3       0.00      0.00      0.00         5\n",
      "           4       0.00      0.00      0.00         8\n",
      "           5       0.00      0.00      0.00         5\n",
      "           6       0.00      0.00      0.00         5\n",
      "           7       0.00      0.00      0.00         5\n",
      "           8       0.00      0.00      0.00         4\n",
      "           9       0.00      0.00      0.00         2\n",
      "          10       0.00      0.00      0.00         1\n",
      "          11       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.27        75\n",
      "   macro avg       0.05      0.11      0.06        75\n",
      "weighted avg       0.13      0.27      0.17        75\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_and_print_metrics(trainer, dataset):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the provided dataset and prints key metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - trainer: Hugging Face Trainer instance with the trained model.\n",
    "    - dataset: Dataset to evaluate (usually the test set).\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing accuracy, F1-score, and classification report.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Use the trainer to predict the labels for the dataset.\n",
    "    predictions = trainer.predict(dataset)\n",
    "\n",
    "    # Step 2: Extract logits and convert them to predicted labels.\n",
    "    logits = predictions.predictions  # Raw model outputs\n",
    "    predicted_labels = np.argmax(logits, axis=1)  # Predicted class indices\n",
    "\n",
    "    # Step 3: Extract the true labels from the dataset.\n",
    "    true_labels = np.array([item['labels'].item() for item in dataset])\n",
    "\n",
    "    # Step 4: Calculate key metrics.\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    # Print the metrics.\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1-Score (Weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "\n",
    "    # Return the metrics for further analysis if needed.\n",
    "    return {\"accuracy\": accuracy, \"f1_score\": f1, \"classification_report\": report}\n",
    "\n",
    "# Example usage:\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "test_metrics = evaluate_and_print_metrics(trainer, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Query: What SMS messages were sent from suspicious phones to 0549876543 containing the word 'urgent'?\n",
      "Predicted Label: Person\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Optional: Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def predict(query, tokenizer, model, label_mapping):\n",
    "    \"\"\"\n",
    "    Perform inference on a given query and return the predicted label.\n",
    "\n",
    "    Parameters:\n",
    "    - query: The user query (string) to predict.\n",
    "    - tokenizer: The ALBERT tokenizer instance.\n",
    "    - model: The trained ALBERT model instance.\n",
    "    - label_mapping: Dictionary to map numeric labels back to original class names.\n",
    "\n",
    "    Returns:\n",
    "    - predicted_label: The predicted label as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 2: Tokenize the input query\n",
    "    inputs = tokenizer(\n",
    "        query,\n",
    "        truncation=True,            # Truncate if input is too long\n",
    "        padding=\"max_length\",        # Pad to the max length\n",
    "        max_length=128,              # Ensure consistent length\n",
    "        return_tensors=\"pt\"          # Return as PyTorch tensors\n",
    "    )\n",
    "\n",
    "    # Move inputs to the appropriate device (GPU/CPU)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Step 3: Perform inference (disable gradient calculation for speed)\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "    # Step 4: Convert logits to predicted class index\n",
    "    predicted_class_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Step 5: Map the predicted class index to the label\n",
    "    predicted_label = label_mapping[predicted_class_idx]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example label mapping (this should match what you used during training)\n",
    "label_mapping = {\n",
    "    0: \"CDR|Phone\",\n",
    "    1: \"CDR\",\n",
    "    2: \"Investigation\",\n",
    "    3: \"Report\",\n",
    "    4: \"Web Activity\",\n",
    "    5: \"Person\",\n",
    "    6: \"EVisa Request\",\n",
    "    7: \"Insight\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Query: who uses 3g\n",
      "Predicted Label: Insight\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the inference function\n",
    "query = \"who uses 3g\"\n",
    "predicted_label = predict(query, tokenizer, model, label_mapping)\n",
    "\n",
    "print(f\"Input Query: {query}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
