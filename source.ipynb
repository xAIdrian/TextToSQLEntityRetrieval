{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: sentencepiece in ./env/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./env/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./env/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./env/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./env/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./env/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./env/lib/python3.11/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !python -m venv env\n",
    "# !source env/bin/activate  \n",
    "!pip install torch transformers scikit-learn pandas sentencepiece\n",
    "!pip install numpy==1.26.4 --force-reinstall\n",
    "# needed for training\n",
    "! pip install -U accelerate\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Case for Getting Familiar with Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample JSON-like data (you'll replace this with your CSV data)\n",
    "json_data = [\n",
    "    {\"entityType\": \"CDR\", \"relationTargetType\": \"Phone\"},\n",
    "    {\"entityType\": \"Report\", \"relationTargetType\": \"Malware\"}\n",
    "]\n",
    "\n",
    "# Example query from the user\n",
    "query = \"What SMS messages were sent from suspicious phones to 0549876543 containing 'urgent'?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to search for relevant entitties in teh JSON data\n",
    "def find_matching_entities(query, json_data):\n",
    "    matching_entities = []\n",
    "\n",
    "    for record in json_data:\n",
    "        entity_text = f\"{record['entityType']} {record['relationTargetType']}\"\n",
    "\n",
    "        #encode inputs for model\n",
    "        inputs = tokenizer(query, entity_text, return_tensors=\"pt\")\n",
    "\n",
    "        #run the model to get answer scores\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #get the start and end scores for the answer\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "        #extract the answer\n",
    "        predicted_entity = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    "        )\n",
    "\n",
    "        #if predicted entity is not empty, consider it a match\n",
    "        if predicted_entity.strip():\n",
    "            return record['entityType'], record['relationTargetType']\n",
    "\n",
    "    return None\n",
    "    # return list(set(matching_entities)) #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test example\n",
    "matching_entities = list(set(find_matching_entities(query, json_data)))\n",
    "print(matching_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  entity_name                   field_name field_type  \\\n",
      "0         CDR      ifc.ootb.CDR.callStatus     string   \n",
      "1         CDR             ifc.CDR.caseCode     string   \n",
      "2         CDR            ifc.CDR.chatTopic     string   \n",
      "3         CDR  ifc.ootb.CDR.createDateTime       date   \n",
      "4         CDR       ifc.ootb.CDR.direction     string   \n",
      "\n",
      "                                         description  \n",
      "0  Status of the call: \"Successful\", \"Failed\", \"B...  \n",
      "1            Unique code identifying a specific case  \n",
      "2         Topic or subject of discussion in the chat  \n",
      "3                  Date and time of record creation.  \n",
      "4         Direction of the call (incoming, outgoing)  \n",
      "                                            question  \\\n",
      "0           Find all calls made using 3G technology.   \n",
      "1  List all Reddit comments posted yesterday with...   \n",
      "2  Show me investigations that are either open or...   \n",
      "3  Find all insights related to the witness Jane ...   \n",
      "4  List all web activities updated in the last da...   \n",
      "\n",
      "                                                json  \n",
      "0  {'entityType': 'CDR', 'statements': [{'type': ...  \n",
      "1  {'entityType': 'Web Activity', 'statements': [...  \n",
      "2  {'entityType': 'Investigation', 'statements': ...  \n",
      "3  {'entityType': 'Insight', 'statements': [{'typ...  \n",
      "4  {'entityType': 'Web Activity', 'statements': [...  \n",
      "test sample mapping\n",
      "[{'field_name': 'ifc.ootb.Participant.isSuspicious', 'description': 'Boolean auto-generated Indicator showing if the identifier was defined as suspicious by the users or is it a SIM-Swapper (defined by the platform)'}, {'field_name': 'ifc.ootb.Participant.targetName', 'description': 'this is target based SIGINT - (local police level) (unlike mass sigint in country level). Name of target like Yossi cohen'}, {'field_name': 'ifc.ootb.Phone.IMEI', 'description': 'International Mobile Equipment Identity of the phone. It is a unique 15-digit code. Each IMEI number is unique to a device and does not change, even if the SIM card is changed'}, {'field_name': 'ifc.ootb.Phone.IMSI', 'description': 'IMSI stands for International Mobile Subscriber Identity. It is a unique number associated with all cellular networks. It is used to identify the user of a cellular network and is a key part of any mobile network. The IMSI is stored on a SIM card and consists of up to 15 digits.The structure of the IMSI is as follows:Mobile Country Code (MCC): The first three digits represent the country code, identifying the country in which the carrier operates.Mobile Network Code (MNC): The next two or three digits identify the mobile network within the country.Mobile Subscriber Identification Number (MSIN): The remainder of the IMSI is a unique identifier for the subscriber within the network.'}, {'field_name': 'ifc.ootb.Phone.MSISDN', 'description': \"MSISDN or phone number is the phone number associated with a mobile network subscriber. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to make and receive calls, send and receive SMS, and utilize mobile data.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).\"}, {'field_name': 'ifc.ootb.Phone.title', 'description': 'Title or label associated with the phone (to display on system)'}, {'field_name': 'ifc.ootb.Participant.tlidDominantLanguage', 'description': 'Dominant language for the Telecommunications Interception Directive'}, {'field_name': 'ifc.ootb.Participant.aliasName', 'description': 'Alias name of the participant'}, {'field_name': 'ifc.ootb.Participant.firstActiveDate', 'description': 'Date when the participant was first active on SIGINT'}, {'field_name': 'ifc.ootb.Participant.lastActiveDate', 'description': 'Date when the participant was last active on SIGINT'}, {'field_name': 'ifc.Phone.accessCallingNumber', 'description': 'Calling number used to access the phone'}, {'field_name': 'ifc.ootb.Participant.deviceName', 'description': 'Name of the device used by the participant'}, {'field_name': 'ifc.ootb.Participant.isTarget', 'description': 'Indicates whether the identifier defined as a target, meaning content is visible (Under a court order)'}, {'field_name': 'ifc.Phone.type', 'description': 'Type of phone (either foreign, mobile, or landline)'}, {'field_name': 'ifc.ootb.Phone.IMSI.objectID', 'description': \"Object ID associated with the phone's IMSI\"}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nd/z10gkxp14_vbcbgjmpzd69200000gn/T/ipykernel_16637/2222115628.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  entity_to_field_mapping = fields_desc.groupby('entity_name').apply(lambda x: x[['field_name', 'description']].to_dict(orient='records')).to_dict()\n"
     ]
    }
   ],
   "source": [
    "# load data and create mapping into new dataframe\n",
    "# right now we are just using the user query \n",
    "### TODO files are static paths now.  we need to make them dynamic and maybe add a nice UI to select the file\n",
    "import pandas as pd\n",
    "\n",
    "fields_desc = pd.read_csv('fields_description.csv')\n",
    "user_queries = pd.read_csv('user_queries.csv')\n",
    "\n",
    "print(fields_desc.head())\n",
    "print(user_queries.head())\n",
    "\n",
    "# Create a dictionary mapping entity names to their field descriptions and properties\n",
    "# This groups the data by entity_name and creates a nested dictionary structure for easy access to field information for each entity type\n",
    "entity_to_field_mapping = fields_desc.groupby('entity_name').apply(lambda x: x[['field_name', 'description']].to_dict(orient='records')).to_dict()\n",
    "\n",
    "print('test sample mapping')\n",
    "print(entity_to_field_mapping.get('Phone', []))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare our data for training. we combine our user query with field description\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    # Remove any leading/trailing whitespace\n",
    "    json_string = json_string.strip()\n",
    "    \n",
    "    # Ensure the string is enclosed in curly braces\n",
    "    if not json_string.startswith('{'):\n",
    "        json_string = '{' + json_string\n",
    "    if not json_string.endswith('}'):\n",
    "        json_string = json_string + '}'\n",
    "    \n",
    "    # Replace single quotes with double quotes, but not within values\n",
    "    json_string = re.sub(r\"(?<!\\\\)'\", '\"', json_string)\n",
    "    \n",
    "    # Remove any trailing commas before closing braces or brackets\n",
    "    json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "    \n",
    "    return json_string\n",
    "\n",
    "def prepare_data_for_training(user_query, fields_mapping):\n",
    "    inputs, labels = [], []\n",
    "\n",
    "    for _, row in user_queries.iterrows():\n",
    "        query = row['question']\n",
    "        cleaned_json_string = clean_json_string(row['json'])\n",
    "        \n",
    "        try:\n",
    "            json_data = json.loads(cleaned_json_string)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON for query: {query}\")\n",
    "            print(f\"Error: {e}\")\n",
    "            continue  # Skip this row and continue with the next one\n",
    "        \n",
    "        # extract entity types and relation target types\n",
    "        entity_type = json_data.get('entityType', '')\n",
    "        relation_type = json_data.get('relationTargetType', '')\n",
    "\n",
    "        # get the description for each entity type\n",
    "        fields = fields_mapping.get(entity_type, [])\n",
    "        field_descriptions = ';'.join([f\"{field['field_name']}: {field['description']}\" for field in fields])\n",
    "\n",
    "        #combine query with descriptions\n",
    "        input_text = f\"Query: {query}. Entity: {entity_type}. Fields: {field_descriptions}\"\n",
    "        inputs.append(input_text)\n",
    "        labels.append(entity_type if not relation_type else f\"{entity_type}|{relation_type}\")\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "# prepare data\n",
    "inputs, labels = prepare_data_for_training(user_queries, entity_to_field_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(f\"Input {i+1}: {inputs[i]}\")\n",
    "    print(f\"Label {i+1}: {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n",
    "    inputs, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training size: {len(train_inputs)}, Validation size: {len(val_inputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "import torch\n",
    "\n",
    "model_name = \"twmkn9/albert-base-v2-squad2\"\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "train_encodings = tokenizer(train_inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# labels to tensors. we can't have names only machine values\n",
    "# remove duplicates and iterate through to assign a number\n",
    "unique_labels = list(set(labels))\n",
    "train_labels_tensor = torch.tensor([unique_labels.index(lbl) for lbl in train_labels])\n",
    "val_labels_tensor = torch.tensor([unique_labels.index(lbl) for lbl in val_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do our dimensions match?\n",
    "print(f\"Training encodings: {train_encodings['input_ids'].shape}, Labels: {train_labels_tensor.shape}\")\n",
    "print(f\"Validation encodings: {val_encodings['input_ids'].shape}, Labels: {val_labels_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects for training and validation\n",
    "train_dataset = EntityDataset(train_encodings, train_labels_tensor)\n",
    "val_dataset = EntityDataset(val_encodings, val_labels_tensor)\n",
    "\n",
    "# check the first sample from the training dataset\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare results folder\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a unique output directory\n",
    "base_output_dir = \"./results\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(base_output_dir, f\"run_{current_time}\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle states of our data\n",
    "import pickle\n",
    "\n",
    "# Save training dataset\n",
    "with open(\"train_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "\n",
    "# Save validation dataset\n",
    "with open(\"val_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(val_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model and training run\n",
    "# check for local first\n",
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# output_dir = \"./results/run_20241014_163059\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    "    no_cuda=True,\n",
    ")\n",
    "\n",
    "# Assuming Training is already done and we have models locally or configuration vars\n",
    "if os.path.exists(output_dir) and os.path.exists(output_dir + \"/spiece.model\"):\n",
    "    print(f\"Loading model from {output_dir}\")\n",
    "    model = AlbertForSequenceClassification.from_pretrained(output_dir)\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "    print(f\"Model loaded from {output_dir}\")\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Reload training dataset\n",
    "    with open(\"train_dataset.pkl\", \"rb\") as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "\n",
    "    # Reload validation dataset\n",
    "    with open(\"val_dataset.pkl\", \"rb\") as f:\n",
    "        val_dataset = pickle.load(f)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "else:\n",
    "    print(f\"Training new model\")\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "should_save = False\n",
    "if should_save:\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "else:\n",
    "    print(f\"No Trainer that's ok. We just won't save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_model(trainer, val_dataset, unique_labels):\n",
    "    # get predictions\n",
    "    preds = trainer.predict(val_dataset)\n",
    "\n",
    "    # covert ML output to labels\n",
    "    preds_labels = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "\n",
    "    # extract TRUE labels\n",
    "    true_labels = [val_dataset[i]['labels'].item() for i in range(len(val_dataset))]\n",
    "\n",
    "    #compute F1 score\n",
    "    f1 = f1_score(true_labels, preds_labels, average='weighted')\n",
    "\n",
    "    # print report\n",
    "    print(f\"weighted f1 score: {f1}\")\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(true_labels, preds_labels, target_names=unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(trainer, val_dataset, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on trained model\n",
    "def infer(model, tokenizer, query, unique_labels):\n",
    "    # Tokenize the input query\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted label ID\n",
    "    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    # Convert the label ID back to the original label name\n",
    "    predicted_label = unique_labels[predicted_label_id]\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the inference function with a sample query\n",
    "sample_query = \"Find all calls made using 4G technology.\"\n",
    "predicted_entity = infer(model, tokenizer, sample_query, unique_labels)\n",
    "\n",
    "print(f\"Predicted Entity for Query: {sample_query}\")\n",
    "print(f\"Predicted Entity Type: {predicted_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 --force-reinstall\n",
    "!pip show numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique labels in your dataset\n",
    "print(f\"Unique Labels: {unique_labels}\")\n",
    "print(f\"Number of Classes (num_labels): {len(unique_labels)}\")\n",
    "\n",
    "# Convert labels to integers from 0 to len(unique_labels) - 1\n",
    "label_ids = torch.tensor([unique_labels.index(lbl) for lbl in labels])\n",
    "\n",
    "# Verify label IDs are within range\n",
    "print(f\"Label IDs: {label_ids}\")\n",
    "print(f\"Max Label ID: {label_ids.max()}, Expected: {len(unique_labels) - 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
