{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep an eye on the progress in notion \n",
    "\n",
    "@October 14 https://www.notion.so/adrianmohnacs/3dcf69ad6da44496ab9f890044158553?pvs=4\n",
    "\n",
    "@October 15\n",
    "\n",
    "## Pipeline Steps:\n",
    "\n",
    "- Depenencies and file loading\n",
    "- Getting familiar with data\n",
    "- Data preparation. This involves mapping the entity type to fields and combining the user query with the field description. (field_Description provides the semantic context to get the correct json prop)\n",
    "- JSON cleaning and error handling\n",
    "- We then label the data using the json entities extracted to provide more context to the data.\n",
    "- Map dictionairies to dataset (this will change)\n",
    "- Training / validation split\n",
    "- Saving dataset locally\n",
    "- Loading or training the a new model\n",
    "- Save the model and tokenizer locally\n",
    "- Evaluation\n",
    "- Inference\n",
    "\n",
    "##  Merged data set for processing\n",
    "**To train we remove the json and field_name using those for labeling**\n",
    "| entity_name | json | field_name | field_type | description |\n",
    "|-------------|------|------------|------------|--------------------------------------------------------------|\n",
    "| CDR         | {'entityType': 'CDR', 'statements': [{'type': 'technology', 'value': '3G'}]} | ifc.ootb.CDR.callStatus | string | Status of the call: \"Successful\", \"Failed\", \"Busy\", etc. |\n",
    "| CDR         | {'entityType': 'Web Activity', 'statements': [{'type': 'platform', 'value': 'Reddit'}, {'type': 'time', 'value': 'yesterday'}, {'type': 'keyword', 'value': 'funny'}]} | ifc.CDR.caseCode | string | Unique code identifying a specific case |\n",
    "| CDR         | {'entityType': 'Investigation', 'statements': [{'type': 'status', 'value': ['open', 'closed']}]} | ifc.CDR.chatTopic | string | Topic or subject of discussion in the chat |\n",
    "| CDR         | {'entityType': 'Insight', 'statements': [{'type': 'relatedTo', 'value': 'Jane Doe'}]} | ifc.ootb.CDR.createDateTime | date | Date and time of record creation. |\n",
    "| CDR         | {'entityType': 'Web Activity', 'statements': [{'type': 'time', 'value': 'last day'}]} | ifc.ootb.CDR.direction | string | Direction of the call (incoming, outgoing) |\n",
    "\n",
    "## Model Selection\n",
    "#### The winner is Albert\n",
    "\n",
    "| Model Variant |\tNumber of Parameters |\tModel Size on Disk\n",
    "|---------------|---------------------|---------------------|\n",
    "albert-base-v2 |\t11M |\t~46 MB |\t12\t|\n",
    "\n",
    "**We then move to training**\n",
    "By combining the query with field descriptions, the model can better understand the semantic meaning of the entities involved, improving its ability to map queries to the correct JSON labels.\n",
    "\n",
    "Input for training Example:\n",
    "\n",
    "```\n",
    "Query: Find all calls made using 3G technology. \n",
    "Entity (Label): CDR. \n",
    "Fields: callStatus: Status of the call; createDateTime: Date and time of record creation.\n",
    "```\n",
    "Label Example:\n",
    "```\n",
    "\"CDR\"\n",
    "```\n",
    "If there is a relation target:\n",
    "```\n",
    "\"CDR|Phone\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](B_DSKq.gif \"segment\")\n",
    "#### Not helping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TLDR\n",
    "\n",
    "We were able to get a local model pipeline in place with evaluation and inference. Prompt engineering is not needed.  Much simpler implementation than LangChain, vector stores, or RAG.\n",
    "\n",
    "This is a good start. We get some great results but the size of the dataset risks overfitting.  \n",
    "\n",
    "Though I will need another day of deep work to get the pipeline to production quality and A LOT OF TESTING.  This will also allow me to add some niceties to the notebook so you can just \"plug and play\".\n",
    "\n",
    "To improve our output I'd like to get clear on what quality outputs look like and discuss the data architecture and the relations between all the features. I'd also like to see more example outputs to get a better sense of the model's desired behavior.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Let's clearly break down the relation between the user query and the fields.  I want to hear it entirely from your perspective?\n",
    "- We want to predict BOTH the entity type and the relation target type?\n",
    "- What do you consider a good output here for the prediction?\n",
    "\n",
    "### What's left to do?\n",
    "\n",
    "- Test cases aligned with the examples you'd like to see\n",
    "- Clean up the notebook and add comments\n",
    "- Look into ways to get a bit more realistic accuracy of the model\n",
    "- Allow for a new CSV to be uploaded in markdown Ui in notebook and assigned to a path variable that is processed in data step\n",
    "- Simplify model training configuration make sure everything is optimized to be run on the local machine and using steps, not a mix of steps and epochs.\n",
    "- Optimized local storage and defensive code to conserve resources and make sure we save and load properly\n",
    "- TEST CASES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./env/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: sentencepiece in ./env/lib/python3.11/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./env/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in ./env/lib/python3.11/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./env/lib/python3.11/site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./env/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./env/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_10_9_x86_64.whl (20.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-1.26.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: accelerate in ./env/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in ./env/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in ./env/lib/python3.11/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in ./env/lib/python3.11/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./env/lib/python3.11/site-packages (from accelerate) (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in ./env/lib/python3.11/site-packages (from accelerate) (0.25.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "# !python -m venv env\n",
    "# !source env/bin/activate  \n",
    "!pip install torch transformers scikit-learn pandas sentencepiece\n",
    "!pip install numpy==1.26.4 --force-reinstall\n",
    "# needed for training\n",
    "! pip install -U accelerate\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Case for Getting Familiar with Data and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample JSON-like data (you'll replace this with your CSV data)\n",
    "json_data = [\n",
    "    {\"entityType\": \"CDR\", \"relationTargetType\": \"Phone\"},\n",
    "    {\"entityType\": \"Report\", \"relationTargetType\": \"Malware\"}\n",
    "]\n",
    "\n",
    "# Example query from the user\n",
    "query = \"What SMS messages were sent from suspicious phones to 0549876543 containing 'urgent'?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to search for relevant entitties in teh JSON data\n",
    "def find_matching_entities(query, json_data):\n",
    "    matching_entities = []\n",
    "\n",
    "    for record in json_data:\n",
    "        entity_text = f\"{record['entityType']} {record['relationTargetType']}\"\n",
    "\n",
    "        #encode inputs for model\n",
    "        inputs = tokenizer(query, entity_text, return_tensors=\"pt\")\n",
    "\n",
    "        #run the model to get answer scores\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        #get the start and end scores for the answer\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "\n",
    "        #extract the answer\n",
    "        predicted_entity = tokenizer.convert_tokens_to_string(\n",
    "            tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    "        )\n",
    "\n",
    "        #if predicted entity is not empty, consider it a match\n",
    "        if predicted_entity.strip():\n",
    "            return record['entityType'], record['relationTargetType']\n",
    "\n",
    "    return None\n",
    "    # return list(set(matching_entities)) #remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test example\n",
    "matching_entities = list(set(find_matching_entities(query, json_data)))\n",
    "print(matching_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "<a href=\"images/processing-step.png\" target=\"_blank\"> <img src=\"images/processing-step.png\" alt=\"High-level overview of the Preprocessing Step\" style=\"max-width: 740px;\" /></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fields_desc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_name</th>\n",
       "      <th>field_name</th>\n",
       "      <th>field_type</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CDR</td>\n",
       "      <td>ifc.ootb.CDR.callStatus</td>\n",
       "      <td>string</td>\n",
       "      <td>Status of the call: \"Successful\", \"Failed\", \"B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CDR</td>\n",
       "      <td>ifc.CDR.caseCode</td>\n",
       "      <td>string</td>\n",
       "      <td>Unique code identifying a specific case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CDR</td>\n",
       "      <td>ifc.CDR.chatTopic</td>\n",
       "      <td>string</td>\n",
       "      <td>Topic or subject of discussion in the chat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CDR</td>\n",
       "      <td>ifc.ootb.CDR.createDateTime</td>\n",
       "      <td>date</td>\n",
       "      <td>Date and time of record creation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CDR</td>\n",
       "      <td>ifc.ootb.CDR.direction</td>\n",
       "      <td>string</td>\n",
       "      <td>Direction of the call (incoming, outgoing)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_name                   field_name field_type  \\\n",
       "0         CDR      ifc.ootb.CDR.callStatus     string   \n",
       "1         CDR             ifc.CDR.caseCode     string   \n",
       "2         CDR            ifc.CDR.chatTopic     string   \n",
       "3         CDR  ifc.ootb.CDR.createDateTime       date   \n",
       "4         CDR       ifc.ootb.CDR.direction     string   \n",
       "\n",
       "                                         description  \n",
       "0  Status of the call: \"Successful\", \"Failed\", \"B...  \n",
       "1            Unique code identifying a specific case  \n",
       "2         Topic or subject of discussion in the chat  \n",
       "3                  Date and time of record creation.  \n",
       "4         Direction of the call (incoming, outgoing)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data and create mapping into new dataframe\n",
    "# right now we are just using the user query \n",
    "### TODO files are static paths now.  we need to make them dynamic and maybe add a nice UI to select the file\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "fields_desc = pd.read_csv('fields_description.csv')\n",
    "user_queries = pd.read_csv('user_queries.csv')\n",
    "\n",
    "print('fields_desc')\n",
    "fields_desc.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "user_queries\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Find all calls made using 3G technology.</td>\n",
       "      <td>{'entityType': 'CDR', 'statements': [{'type': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>List all Reddit comments posted yesterday with...</td>\n",
       "      <td>{'entityType': 'Web Activity', 'statements': [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Show me investigations that are either open or...</td>\n",
       "      <td>{'entityType': 'Investigation', 'statements': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find all insights related to the witness Jane ...</td>\n",
       "      <td>{'entityType': 'Insight', 'statements': [{'typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>List all web activities updated in the last da...</td>\n",
       "      <td>{'entityType': 'Web Activity', 'statements': [...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0           Find all calls made using 3G technology.   \n",
       "1  List all Reddit comments posted yesterday with...   \n",
       "2  Show me investigations that are either open or...   \n",
       "3  Find all insights related to the witness Jane ...   \n",
       "4  List all web activities updated in the last da...   \n",
       "\n",
       "                                                json  \n",
       "0  {'entityType': 'CDR', 'statements': [{'type': ...  \n",
       "1  {'entityType': 'Web Activity', 'statements': [...  \n",
       "2  {'entityType': 'Investigation', 'statements': ...  \n",
       "3  {'entityType': 'Insight', 'statements': [{'typ...  \n",
       "4  {'entityType': 'Web Activity', 'statements': [...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('\\nuser_queries')\n",
    "user_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>744</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>742</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Show me insights where the text includes 'witn...</td>\n",
       "      <td>{'entityType': 'Phone', 'statements': [{'type'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "count                                                 744   \n",
       "unique                                                742   \n",
       "top     Show me insights where the text includes 'witn...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                     json  \n",
       "count                                                 744  \n",
       "unique                                                721  \n",
       "top     {'entityType': 'Phone', 'statements': [{'type'...  \n",
       "freq                                                    3  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fields_desc.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>744</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>742</td>\n",
       "      <td>721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Show me insights where the text includes 'witn...</td>\n",
       "      <td>{'entityType': 'Phone', 'statements': [{'type'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "count                                                 744   \n",
       "unique                                                742   \n",
       "top     Show me insights where the text includes 'witn...   \n",
       "freq                                                    2   \n",
       "\n",
       "                                                     json  \n",
       "count                                                 744  \n",
       "unique                                                721  \n",
       "top     {'entityType': 'Phone', 'statements': [{'type'...  \n",
       "freq                                                    3  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_queries.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " value distribution\n",
      "entity_name\n",
      "CDR              36\n",
      "EVisa Request    29\n",
      "Web Activity     25\n",
      "Web Actor        23\n",
      "Phone            15\n",
      "Person            6\n",
      "Investigation     5\n",
      "Report            3\n",
      "Insight           2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('value distribution for field entities')\n",
    "print(fields_desc['entity_name'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ifc.ootb.CDR.callStatus': 'Status of the call: \"Successful\", \"Failed\", \"Blocked\", or \"Redirected\"', 'ifc.CDR.caseCode': 'Unique code identifying a specific case', 'ifc.CDR.chatTopic': 'Topic or subject of discussion in the chat', 'ifc.ootb.CDR.createDateTime': 'Date and time of record creation.', 'ifc.ootb.CDR.direction': 'Direction of the call (incoming, outgoing)', 'ifc.ootb.CDR.duration': 'Duration of the communication in minutes. You can ask it For example: 1min -> 60', 'ifc.CDR.emailSubject': 'Subject of the email communication', 'ifc.ootb.CDR.endTime': 'Time when the communication ended', 'ifc.ootb.CDR.hasContent': 'Indicates if the communication has content', 'ifc.ootb.CDR.imei': 'this field is intended to store the IMEI number of the device who made a call.', 'ifc.ootb.CDR.imei2': \"this field is intended to store the IMEI number of the device that is receiving the call. It serves the same purposes as the caller's IMEI but for the receiving side of the communication.\", 'ifc.ootb.CDR.imsi': 'The IMSI that initiate the call as a sender. IMSI stands for International Mobile Subscriber Identity. It is a unique number associated with all cellular networks. It is used to identify the user of a cellular network and is a key part of any mobile network. The IMSI is stored on a SIM card and consists of up to 15 digits.The structure of the IMSI is as follows:Mobile Country Code (MCC): The first three digits represent the country code, identifying the country in which the carrier operates.Mobile Network Code (MNC): The next two or three digits identify the mobile network within the country.Mobile Subscriber Identification Number (MSIN): The remainder of the IMSI is a unique identifier for the subscriber within the network.', 'ifc.ootb.CDR.imsi2': 'The IMSI that received the call as a receiver. IMSI is an International Mobile Subscriber Identity (IMSI) is a 15-digit number for every user in a Global System for Mobile communication (GSM). The IMSI is used by Mobile Network Operators (MNOs) and is an important part of the Subscriber Identity Module (SIM) profile.', 'ifc.CDR.interceptionCriteria': 'Criteria for intercepting the communication', 'ifc.CDR.interceptionFilter': 'Filter applied for interception', 'ifc.ootb.CDR.ipAddress': \"IP address of the communication's  Origin \", 'ifc.ootb.CDR.isAttachment': 'Indicates if the communication has an attachment', 'ifc.CDR.isClassified': 'Indicates if the communication is classified', 'ifc.ootb.CDR.msisdn': \"MSISDN or phone number is the phone number associated with a mobile network subscriber as a sender. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to make calls, send SMS, and utilize mobile data as sender.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).\", 'ifc.ootb.CDR.msisdn2': \"MSISDN or phone number is the phone number associated with a mobile network subscriber as a receiver. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to receive calls, receive SMS, and utilize mobile data as reciever.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).\", 'ifc.ootb.CDR.protocol': 'Communication protocol used like GSM or UMTS', 'ifc.ootb.CDR.provider': 'Provider of the network service', 'ifc.ootb.CDR.smsProtocol': 'Protocol used for SMS communication', 'ifc.ootb.CDR.smsText': 'Text content of the SMS', 'ifc.ootb.CDR.smsType': 'Type of the SMS (e.g., text, binary)', 'ifc.ootb.CDR.startTime': 'Time when the communication started', 'ifc.CDR.subcaseCode': 'Code for the subcase of a known case. Identify between this field and the ifc.CDR.caseCode', 'ifc.CDR.targetCode': 'Code identifying the target of the communication', 'ifc.ootb.CDR.technology': 'Technology used in the communication, either: 2G,3G,4G,5G,other, WiFi', 'ifc.ootb.CDR.type': 'Type of the communication, either: None-Call, Text,Voice, VoiceEdited, Web or Email. Text type is SMS, Voice type is Phone Calls', 'ifc.ootb.CDR.mainLanguage': 'Primary language of the communication content', 'ifc.ootb.CDR.ipAddress2': \"IP address of the communication's  Destination\", 'ifc.ootb.CDR.originatorEmailAddress': 'Email address of the sender', 'ifc.ootb.CDR.destinationEmailAddress': 'Email address of the recipient', 'ifc.ootb.CDR.orignatorNetworkName': \"Name of the originator's network\", 'ifc.ootb.CDR.destinatorNetworkName': \"Name of the recipient's network\", 'ifc.ootb.Orbis.platform': 'the social network name form which the actor was collected (e.g. facebook instagram linkedin etc.) ', 'ifc.ootb.WebActivity.writerFullName': 'Full name of the writer who wrote the post', 'ifc.ootb.WebActivity.writerUri': 'URI of the writers profile or content', 'ifc.ootb.Orbis.updateDate': 'Date when the post was updated', 'ifc.ootb.Orbis.collectDate': 'Date when the web actor was last collected ', 'ifc.ootb.Orbis.publishDate': 'Date when the web actor was published or created in the social network', 'ifc.ootb.WebActivity.numberOfLikes': 'Number of likes on the activity', 'ifc.ootb.WebActivity.numberShares': 'Number of times the activity was shared', 'ifc.ootb.WebActivity.numberActivities': 'Count of all web activities (e.g. posts, reshares) nested under this web activity', 'ifc.ootb.WebActivity.numberTags': 'Number of tags associated with the activity', 'ifc.ootb.WebActivity.mainLanguage': 'Main language of the activity', 'ifc.ootb.WebActivity.sentiment': 'sentiment of the text with the web activity (can be positive or negative)', 'ifc.ootb.WebActivity.sentimentScore': 'a score between 0 (low) and 1 (high) of the sentiment analysis certainty of the text of the activity', 'ifc.ootb.WebActivity.text': 'Text content of the web activity', 'ifc.ootb.Orbis.creditCard': 'NER extraction of entities representing credit card information as extracted from ORBIS', 'ifc.ootb.Orbis.email': 'NER extraction of entities representing emails as extracted from ORBIS', 'ifc.ootb.Orbis.hashtags': 'Hashtags mentioned in the activity text', 'ifc.ootb.Orbis.ipAddress': 'IP address mentioned in the activity text', 'ifc.ootb.Orbis.mentions': 'a mention of a profile in social media usually with an @ sign', 'ifc.ootb.Orbis.nationality': 'Nationality mentioned in the activity text', 'ifc.ootb.Orbis.phone': 'Phone number mentioned in the activity text', 'ifc.ootb.Orbis.religion': 'Religions mentioned in the activity text', 'ifc.ootb.Orbis.url': 'URL mentioned in the activity text', 'ifc.ootb.Orbis.landmarks': 'names of landmarks found in the activity (e.g eiffel tower)', 'ifc.ootb.WebActivity.title': 'Title of the web activity', 'ifc.core.description': 'Detailed summary of the investigation', 'ifc.core.dueTime': 'Scheduled completion time for the investigation', 'ifc.core.state': 'Current state of the investigation. either \"open\" or \"close\"', 'ifc.core.priority': 'Priority level of the investigation: \"regular\", \"medium\" or \"high\" in ascending order', 'ifc.core.createtime': 'Creation time of the report', 'text': 'Full text content of the report', 'ifc.ootb.Participant.isSuspicious': 'Boolean auto-generated Indicator showing if the identifier was defined as suspicious by the users or is it a SIM-Swapper (defined by the platform)', 'ifc.ootb.Participant.targetName': 'this is target based SIGINT - (local police level) (unlike mass sigint in country level). Name of target like Yossi cohen', 'ifc.ootb.Phone.IMEI': 'International Mobile Equipment Identity of the phone. It is a unique 15-digit code. Each IMEI number is unique to a device and does not change, even if the SIM card is changed', 'ifc.ootb.Phone.IMSI': 'IMSI stands for International Mobile Subscriber Identity. It is a unique number associated with all cellular networks. It is used to identify the user of a cellular network and is a key part of any mobile network. The IMSI is stored on a SIM card and consists of up to 15 digits.The structure of the IMSI is as follows:Mobile Country Code (MCC): The first three digits represent the country code, identifying the country in which the carrier operates.Mobile Network Code (MNC): The next two or three digits identify the mobile network within the country.Mobile Subscriber Identification Number (MSIN): The remainder of the IMSI is a unique identifier for the subscriber within the network.', 'ifc.ootb.Phone.MSISDN': \"MSISDN or phone number is the phone number associated with a mobile network subscriber. It is the globally unique number that identifies a subscriber's subscription in a mobile network, allowing them to make and receive calls, send and receive SMS, and utilize mobile data.Purpose: The primary purpose of an MSISDN is to identify the subscriber's phone number for routing calls and messages. It is tied to the SIM card (Subscriber Identity Module) used in the mobile device.Components: An MSISDN includes the country code (CC), the national destination code (NDC), and the subscriber number (SN).\", 'ifc.ootb.Phone.title': 'Title or label associated with the phone (to display on system)', 'ifc.ootb.Participant.tlidDominantLanguage': 'Dominant language for the Telecommunications Interception Directive', 'ifc.ootb.Participant.aliasName': 'Alias name of the participant', 'ifc.ootb.Participant.firstActiveDate': 'Date when the participant was first active on SIGINT', 'ifc.ootb.Participant.lastActiveDate': 'Date when the participant was last active on SIGINT', 'ifc.Phone.accessCallingNumber': 'Calling number used to access the phone', 'ifc.ootb.Participant.deviceName': 'Name of the device used by the participant', 'ifc.ootb.Participant.isTarget': 'Indicates whether the identifier defined as a target, meaning content is visible (Under a court order)', 'ifc.Phone.type': 'Type of phone (either foreign, mobile, or landline)', 'ifc.ootb.Phone.IMSI.objectID': \"Object ID associated with the phone's IMSI\", 'ifc.ootb.Orbis.sourceUrl': 'Source URL from where the web actors data is retrieved', 'ifc.ootb.WebActor.title': 'name of the web actor', 'ifc.ootb.WebActor.uri': 'the url identifying the web actor within the social network', 'ifc.ootb.WebActor.sentiment': 'sentiment of the text field within the webactor  (can be positive or negative)', 'ifc.ootb.WebActor.sentimentScore': 'a score between 0 (low) and 1 (high) of the sentiment analysis certainty of  he text field within the webactor  ', 'ifc.ootb.WebActor.numberOfFriends': 'Number of friends or connections the web actor has', 'ifc.ootb.WebActor.numberActivities': 'Total number of activities the web actor has performed', 'ifc.ootb.WebActor.numberPagesLiked': 'Number of pages or profiles the actor has liked', 'ifc.ootb.WebActor.educationalInstitution': 'Educational institutions associated with the actor', 'ifc.ootb.WebActor.jobCompanyName': 'Names of companies where the actor is or has been employed', 'ifc.ootb.WebActor.leadingEmail': 'Primary email address associated with the actor', 'ifc.ootb.WebActor.homepage': 'URL which the web actor published as his home page', 'ifc.ootb.WebActor.about': 'the description of the web actor as he wrote on himself inside his social network profile', 'ifc.ootb.WebActor.birthDate': 'Birth date of the individual web actor', 'ifc.ootb.WebActor.gender': 'Gender of the web actor', 'ifc.ootb.WebActor.firstName': 'First name of the individual web actor', 'ifc.ootb.WebActor.lastName': 'Last name of the individual web actor', 'ifc.ootb.WebActor.leadingPhone': 'Primary phone number of the actor', 'ifc.ootb.WebActor.numberOfFollowers': 'Number of followers the actor has', 'ifc.ootb.WebActor.numberOfFollowing': 'Number of web actors the actor is following', 'ifc.core.updatetime': 'Last updated time for the report', 'ifc.Person.passport': 'Passport number of the individual', 'ifc.Person.firstName': 'First name of the individual', 'ifc.Person.lastName': 'Last name of the individual', 'ifc.Person.gender': 'Gender of the individual', 'ifc.Person.birthDate': 'Birth date of the individual', 'ifc.Person.occupation': 'Occupation or profession of the individual', 'ifc.EVisaRequest.title': 'Title or formal designation of the eVisa applicant', 'ifc.EVisaRequest.countryBeforeArrivalName': 'Country the applicant is travelling from', 'ifc.EVisaRequest.countryAfterDepartureName': 'Country the applicant intends to travel to after departure', 'ifc.EVisaRequest.arrivalDate': 'Scheduled date of arrival in the destination country', 'ifc.EVisaRequest.departureDate': 'Scheduled departure date from the destination country', 'ifc.EVisaRequest.state': 'Current state or status of the eVisa application', 'ifc.EVisaRequest.demandDate': 'Date when the eVisa request was made', 'ifc.EVisaRequest.priority': 'Priority level of the eVisa application', 'ifc.EVisaRequest.previousStayinDate': 'Date of previous stays, if any, in the destination country', 'ifc.EVisaRequest.localAddress': 'Local address in the destination country', 'ifc.EVisaRequest.stayinDuration': 'Duration of the intended stay on the eVisa', 'ifc.EVisaRequest.travelPurpose': 'Primary purpose of travel', 'ifc.EVisaRequest.citizenshipCountryName': 'Country of citizenship of the eVisa applicant', 'ifc.EVisaRequest.otherCitizenshipCountryName': 'For applicant that has more than one citizenship, this property helds the additional citizenship', 'ifc.EVisaRequest.fullName': 'Full legal name of the eVisa applicant', 'ifc.EVisaRequest.email': 'Email address of the eVisa applicant', 'ifc.EVisaRequest.contacts': 'Contact details for the eVisa applicant', 'ifc.EVisaRequest.occupation': 'Occupation or job title of the eVisa applicant', 'ifc.EVisaRequest.documentType': 'Type of travel document used for the eVisa application', 'ifc.EVisaRequest.documentNumber': 'Document number of the travel document', 'ifc.EVisaRequest.documentDeliveryDate': 'Date when the travel document was issued', 'ifc.EVisaRequest.documentExpiryDate': 'Expiration date of the travel document', 'ifc.EVisaRequest.documentIssuer': 'Authority that issued the travel document', 'ifc.EVisaRequest.documentIssuerCountryName': 'Country of the authority that issued the travel document', 'ifc.EVisaRequest.visaIssueDate': 'Date when the eVisa was issued', 'ifc.EVisaRequest.visaExpirationDate': 'Expiration date of the issued eVisa', 'ifc.EVisaRequest.visaType': 'Type or category of the issued eVisa', 'ifc.EVisaRequest.visaDuration': 'Validity duration of the eVisa from the date of issue', 'ifc.EVisaRequest.visaEntries': 'Number of entries allowed on the eVisa'}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping field_name â†’ description for quick lookup.\n",
    "# let's just look at this like a dictionary\n",
    "field_mapping = fields_desc.set_index('field_name')['description'].to_dict()\n",
    "print(field_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare our data for training. we combine our user query with field description\n",
    "import json\n",
    "import re\n",
    "\n",
    "def clean_json_string(json_string):\n",
    "    # Remove any leading/trailing whitespace\n",
    "    json_string = json_string.strip()\n",
    "    \n",
    "    # Ensure the string is enclosed in curly braces\n",
    "    if not json_string.startswith('{'):\n",
    "        json_string = '{' + json_string\n",
    "    if not json_string.endswith('}'):\n",
    "        json_string = json_string + '}'\n",
    "    \n",
    "    # Replace single quotes with double quotes, but not within values\n",
    "    json_string = re.sub(r\"(?<!\\\\)'\", '\"', json_string)\n",
    "    \n",
    "    # Remove any trailing commas before closing braces or brackets\n",
    "    json_string = re.sub(r',\\s*([\\]}])', r'\\1', json_string)\n",
    "    \n",
    "    return json.loads(json_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question    Show me all non-voice communications from IMEI...\n",
      "json        {'entityType': 'CDR', 'statements': [{'type': ...\n",
      "Name: 30, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def find_matching_json_for_field_name(field_name, user_queries):\n",
    "    \"\"\"\n",
    "    Finds the first user query where the JSON contains the given field_name\n",
    "    as a parameter in one of its filter statements.\n",
    "    \n",
    "    Parameters:\n",
    "    - field_name: The field name to search for.\n",
    "    - user_queries: DataFrame containing the user queries and their JSON objects.\n",
    "\n",
    "    Returns:\n",
    "    - matching_row: The row from user_queries where the field_name is found.\n",
    "    \"\"\"\n",
    "    for _, row in user_queries.iterrows():\n",
    "        try:\n",
    "            json_data = json.loads(clean_json_string(row['json']))\n",
    "        except json.JSONDecodeError as e:\n",
    "            # print(f\"Error decoding JSON for query: {row['question']}\")\n",
    "            # print(f\"Error: {e}\")\n",
    "            json_data = {}\n",
    "\n",
    "        # Check if any filter statement in the JSON contains the field_name as a parameter\n",
    "        for statement in json_data.get('statements', []):\n",
    "            param_name = statement['parameters'].get('name', '')\n",
    "            if param_name == field_name:\n",
    "                return row  # Return the row if a match is found\n",
    "\n",
    "    return None  # Return None if no matching JSON is found\n",
    "\n",
    "print(find_matching_json_for_field_name('ifc.ootb.CDR.imei', user_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Query: Show failed call attempts to 0506655668...</td>\n",
       "      <td>CDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Query: Find all communications related to case...</td>\n",
       "      <td>CDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Query: List all emails from info@company.co.il...</td>\n",
       "      <td>CDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Query: List successful incoming calls to 05466...</td>\n",
       "      <td>CDR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Query: What 10-minute calls were made from 054...</td>\n",
       "      <td>CDR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text label\n",
       "0  Query: Show failed call attempts to 0506655668...   CDR\n",
       "1  Query: Find all communications related to case...   CDR\n",
       "2  Query: List all emails from info@company.co.il...   CDR\n",
       "3  Query: List successful incoming calls to 05466...   CDR\n",
       "4  Query: What 10-minute calls were made from 054...   CDR"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def enrich_input_with_matching_json(field_mapping, user_queries):\n",
    "    \"\"\"\n",
    "    Iterates over field_mapping and matches each field_name with the JSON in user_queries.\n",
    "    Enriches the input text with the query, entityType, and semantic descriptions.\n",
    "\n",
    "    Parameters:\n",
    "    - field_mapping: Dictionary of field_name â†’ description.\n",
    "    - user_queries: DataFrame containing user queries and their JSON objects.\n",
    "\n",
    "    Returns:\n",
    "    - enriched_data: List of dictionaries with input_text and label for each matching field_name.\n",
    "    \"\"\"\n",
    "\n",
    "    enriched_data = []  # Store the enriched input_text and label pairs\n",
    "\n",
    "    for field_name, description in field_mapping.items():\n",
    "        # Find the matching JSON for the current field_name\n",
    "        matching_row = find_matching_json_for_field_name(field_name, user_queries)\n",
    "\n",
    "        if matching_row is not None:\n",
    "            # Extract the query and JSON from the matching row\n",
    "            query = matching_row['question']\n",
    "            try:\n",
    "                json_data = json.loads(clean_json_string(matching_row['json']))\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON for query: {matching_row['question']}\")\n",
    "                print(f\"Error: {e}\")\n",
    "                json_data = {}\n",
    "\n",
    "            # Extract entityType and relationTargetType from the JSON\n",
    "            entity_type = json_data.get('entityType', '')\n",
    "            relation_type = ''\n",
    "            for statement in json_data.get('statements', []):\n",
    "                if statement['type'] == 'relation':\n",
    "                    relation_type = statement['parameters'].get('relationTargetType', [''])[0]\n",
    "\n",
    "            # Create the label for training\n",
    "            label = entity_type if not relation_type else f\"{entity_type}|{relation_type}\"\n",
    "\n",
    "            # Prepare the input text combining query, field_name, and description\n",
    "            input_text = (\n",
    "                f\"Query: {query}. \"\n",
    "                f\"Field: {field_name}. \"\n",
    "                f\"Description: {description}.\"\n",
    "            )\n",
    "\n",
    "            # Store the enriched input and label\n",
    "            enriched_data.append({\n",
    "                'input_text': input_text,\n",
    "                'label': label\n",
    "            })\n",
    "\n",
    "    return enriched_data\n",
    "\n",
    "# Apply the enrichment function\n",
    "enriched_data = enrich_input_with_matching_json(field_mapping, user_queries)\n",
    "\n",
    "# Convert enriched data into a DataFrame for inspection\n",
    "enriched_df = pd.DataFrame(enriched_data)\n",
    "\n",
    "# Inspect the enriched DataFrame\n",
    "enriched_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value distribution for field entities\n",
      "label\n",
      "CDR                       28\n",
      "Web Activity              20\n",
      "Web Actor                 17\n",
      "Phone                     12\n",
      "EVisa Request             11\n",
      "Person                     6\n",
      "Investigation              4\n",
      "Web Activity|Web Actor     3\n",
      "Insight                    2\n",
      "Phone|CDR                  1\n",
      "Web Actor|Web Activity     1\n",
      "Report                     1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('value distribution for field entities')\n",
    "print(enriched_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 63\n",
      "Validation set size: 21\n",
      "Test set size: 22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert the enriched data into a DataFrame (if not already).\n",
    "enriched_df = pd.DataFrame(enriched_data)\n",
    "\n",
    "# Step 1: Split the data into Train + Validation and Test sets (80% Train+Val, 20% Test).\n",
    "train_val_df, test_df = train_test_split(enriched_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Split the Train + Validation set further into Training and Validation sets (75% Train, 25% Val).\n",
    "train_df, val_df = train_test_split(train_val_df, test_size=0.25, random_state=42)\n",
    "# This ensures 60% Train, 20% Val, 20% Test overall.\n",
    "\n",
    "# Check the sizes of the splits.\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "<a href=\"images/training-step.png\" target=\"_blank\"> <img src=\"images/training-step.png\" alt=\"High-level overview of the Training Step\" style=\"max-width: 740px;\" /></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "import torch\n",
    "\n",
    "def tokenize_data(data, tokenizer):\n",
    "    cleaned_inputs = data['input_text'].fillna(\"\").astype(str).tolist()\n",
    "    encodings = tokenizer(cleaned_inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    # Convert the labels to integers using factorize() and store them as a tensor.\n",
    "    labels = torch.tensor(data['label'].factorize()[0])\n",
    "    return encodings['input_ids'], encodings['attention_mask'], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the train, validation, and test sets.\n",
    "model_name = \"twmkn9/albert-base-v2-squad2\"\n",
    "tokenizer = AlbertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_input_ids, train_attention_mask, train_labels = tokenize_data(train_df, tokenizer)\n",
    "val_input_ids, val_attention_mask, val_labels = tokenize_data(val_df, tokenizer)\n",
    "test_input_ids, test_attention_mask, test_labels = tokenize_data(test_df, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input shape: torch.Size([63, 215])\n",
      "Train attention mask shape: torch.Size([63, 215])\n",
      "Train labels shape: torch.Size([63])\n",
      "Validation input shape: torch.Size([21, 132])\n",
      "Validation attention mask shape: torch.Size([21, 132])\n",
      "Validation labels shape: torch.Size([21])\n",
      "Test input shape: torch.Size([22, 69])\n",
      "Test attention mask shape: torch.Size([22, 69])\n",
      "Test labels shape: torch.Size([22])\n"
     ]
    }
   ],
   "source": [
    "#Verify the shape of the tokenized outputs to ensure they match expectations.\n",
    "print(f\"Train input shape: {train_input_ids.shape}\")\n",
    "print(f\"Train attention mask shape: {train_attention_mask.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "\n",
    "print(f\"Validation input shape: {val_input_ids.shape}\")\n",
    "print(f\"Validation attention mask shape: {val_attention_mask.shape}\")\n",
    "print(f\"Validation labels shape: {val_labels.shape}\")\n",
    "\n",
    "print(f\"Test input shape: {test_input_ids.shape}\")\n",
    "print(f\"Test attention mask shape: {test_attention_mask.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Step 1: Define a PyTorch Dataset class to handle our tokenized data.\n",
    "class QueryDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset to wrap tokenized data and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch an example by index\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Create instances of QueryDataset for train, validation, and test sets.\n",
    "train_dataset = QueryDataset(train_input_ids, train_attention_mask, train_labels)\n",
    "val_dataset = QueryDataset(val_input_ids, val_attention_mask, val_labels)\n",
    "\n",
    "# Step 3: Load the pre-trained ALBERT model for sequence classification.\n",
    "# num_labels: Number of unique labels in our dataset.\n",
    "num_labels = len(train_labels.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare results folder\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create a unique output directory\n",
    "base_output_dir = \"./results\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = os.path.join(base_output_dir, f\"run_{current_time}\")\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Users/millionairemacmillionairemac/Developer/TextToSQLEntityRetrieval/env/lib/python3.11/site-packages/transformers/training_args.py:1560: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving training dataset\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at twmkn9/albert-base-v2-squad2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [01:51<00:00,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 111.7858, 'train_samples_per_second': 1.691, 'train_steps_per_second': 0.215, 'train_loss': 2.0604159037272134, 'epoch': 3.0}\n",
      "AlbertForSequenceClassification(\n",
      "  (albert): AlbertModel(\n",
      "    (embeddings): AlbertEmbeddings(\n",
      "      (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "    )\n",
      "    (encoder): AlbertTransformer(\n",
      "      (embedding_hidden_mapping_in): Linear(in_features=128, out_features=768, bias=True)\n",
      "      (albert_layer_groups): ModuleList(\n",
      "        (0): AlbertLayerGroup(\n",
      "          (albert_layers): ModuleList(\n",
      "            (0): AlbertLayer(\n",
      "              (full_layer_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (attention): AlbertSdpaAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (attention_dropout): Dropout(p=0, inplace=False)\n",
      "                (output_dropout): Dropout(p=0, inplace=False)\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              )\n",
      "              (ffn): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (ffn_output): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (activation): NewGELUActivation()\n",
      "              (dropout): Dropout(p=0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (pooler_activation): Tanh()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=12, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model and training run\n",
    "# check for local first\n",
    "from transformers import AlbertForSequenceClassification, AlbertTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# uncomment this to use an old model\n",
    "# output_dir = \"./results/run_20241014_163059\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01, # Weight decay to reduce overfitting\n",
    "    no_cuda=True,           \n",
    "    logging_dir=\"./logs\"   \n",
    ")\n",
    "\n",
    "# Assuming Training is already done and we have models locally or configuration vars\n",
    "if os.path.exists(output_dir) and os.path.exists(output_dir + \"/spiece.model\"):\n",
    "    print(f\"Loading model from {output_dir}\")\n",
    "    model = AlbertForSequenceClassification.from_pretrained(output_dir, num_labels = num_labels)\n",
    "    tokenizer = AlbertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "    print(f\"Model loaded from {output_dir}\")\n",
    "    print(f\"Model: {model}\")\n",
    "\n",
    "    # Reload training dataset\n",
    "    with open(\"train_dataset.pkl\", \"rb\") as f:\n",
    "        train_dataset = pickle.load(f)\n",
    "\n",
    "    # Reload validation dataset\n",
    "    with open(\"val_dataset.pkl\", \"rb\") as f:\n",
    "        val_dataset = pickle.load(f)\n",
    "\n",
    "    print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "    print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "else:\n",
    "    print('saving training dataset')\n",
    "    # Save training dataset\n",
    "    with open(f\"{output_dir}/train_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(train_dataset, f)\n",
    "\n",
    "    # Save validation dataset\n",
    "    with open(f\"{output_dir}/val_dataset.pkl\", \"wb\") as f:\n",
    "        pickle.dump(val_dataset, f)\n",
    "\n",
    "    print(f\"Starting training...\")\n",
    "    model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels = num_labels)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of unique labels in your dataset\n",
    "print(f\"Unique Labels: {unique_labels}\")\n",
    "print(f\"Number of Classes (num_labels): {len(unique_labels)}\")\n",
    "# Convert labels to integers from 0 to len(unique_labels) - 1\n",
    "label_ids = torch.tensor([unique_labels.index(lbl) for lbl in labels])\n",
    "\n",
    "# Verify label IDs are within range\n",
    "print(f\"Label IDs: {label_ids}\")\n",
    "print(f\"Max Label ID: {label_ids.max()}, Expected: {len(unique_labels) - 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trainer that's ok. We just won't save\n"
     ]
    }
   ],
   "source": [
    "should_save = False\n",
    "if should_save:\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_dir}\")\n",
    "else:\n",
    "    print(f\"No Trainer that's ok. We just won't save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "def evaluate_model(trainer, val_dataset, unique_labels):\n",
    "    # get predictions\n",
    "    preds = trainer.predict(val_dataset)\n",
    "\n",
    "    # covert ML output to labels\n",
    "    preds_labels = torch.argmax(torch.tensor(preds.predictions), dim=1).numpy()\n",
    "\n",
    "    # extract TRUE labels\n",
    "    true_labels = [val_dataset[i]['labels'].item() for i in range(len(val_dataset))]\n",
    "\n",
    "    #compute F1 score\n",
    "    f1 = f1_score(true_labels, preds_labels, average='weighted')\n",
    "\n",
    "    # print report\n",
    "    print(f\"weighted f1 score: {f1}\")\n",
    "    print(\"Classification Report:\\n\")\n",
    "    print(classification_report(true_labels, preds_labels, target_names=unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [01:44,  1.36s/it]                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weighted f1 score: 1.0\n",
      "Classification Report:\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          CDR       1.00      1.00      1.00        37\n",
      "Investigation       1.00      1.00      1.00        15\n",
      "      Insight       1.00      1.00      1.00        11\n",
      "        Phone       1.00      1.00      1.00        16\n",
      "       Report       1.00      1.00      1.00        14\n",
      "    Web Actor       1.00      1.00      1.00        14\n",
      "       Person       1.00      1.00      1.00        10\n",
      "EVisa Request       1.00      1.00      1.00         2\n",
      " Web Activity       1.00      1.00      1.00        20\n",
      "\n",
      "     accuracy                           1.00       139\n",
      "    macro avg       1.00      1.00      1.00       139\n",
      " weighted avg       1.00      1.00      1.00       139\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(trainer, val_dataset, unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference on trained model\n",
    "def infer(model, tokenizer, query, unique_labels):\n",
    "    # Tokenize the input query\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted label ID\n",
    "    predicted_label_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    # Convert the label ID back to the original label name\n",
    "    predicted_label = unique_labels[predicted_label_id]\n",
    "\n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Entity for Query: Which SMS were rejected?\n",
      "Predicted Entity Type: Phone\n"
     ]
    }
   ],
   "source": [
    "# Test the inference function with a sample query\n",
    "sample_query = \"Which SMS were rejected?\"\n",
    "predicted_entity = infer(model, tokenizer, sample_query, unique_labels)\n",
    "\n",
    "print(f\"Predicted Entity for Query: {sample_query}\")\n",
    "print(f\"Predicted Entity Type: {predicted_entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
